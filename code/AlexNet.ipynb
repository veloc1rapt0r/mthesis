{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AlexNet\n",
    "This notebook was motivated by\n",
    "[1] Alex Krizhevsky, Ilya Sutskever and Geoffrey E Hinton. ‘ImageNet Classification with Deep Convolutional Neural Networks’. In: (2012). url: https://proceedings.neurips.cc/paper/2012/file/c 399862d3b9d6b76c8436e924a68c45b-Paper.pdf.\n",
    "\n",
    "https://papers.nips.cc/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html\n",
    "\n",
    "Implementation: Oleh Bakumenko, Univerity of Duisburg-Essen\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "import os\n",
    "import time\n",
    "import torch, torch.nn as nn\n",
    "import torchvision, torchvision.transforms as tt\n",
    "from torch.multiprocessing import Manager\n",
    "torch.multiprocessing.set_sharing_strategy(\"file_system\")\n",
    "from torchsummary import summary\n",
    "from pathlib import Path\n",
    "\n",
    "from utility import utils as uu\n",
    "from utility.eval import evaluate_classifier_model\n",
    "\n",
    "from utility.trainLoopClassifier import *\n",
    "from utility.plotImageModel import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data augmentations\n",
    "Data augmentation is a technique used to artificially increase the size of a dataset by transforming existing data points to create new, similar instances. This can help prevent overfitting in machine learning models, as well as improve their ability to generalize to unseen data. Common types of data augmentation include flipping, rotation, scaling, and adding noise to images.\n",
    "We can generate the augmentation list with torchvision.transforms module\n",
    "\n",
    "Random croping of the image to the size of 224 will be excluded for other models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augments = torchvision.transforms.Compose([ \n",
    "    torchvision.transforms.RandomHorizontalFlip(p = .5),\n",
    "    torchvision.transforms.RandomVerticalFlip(p = .5),\n",
    "    torchvision.transforms.ColorJitter(brightness=(0.5,1.5), contrast=(1), hue=(-0.1,0.1)),\n",
    "    torchvision.transforms.RandomCrop((224, 224)),\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Load the dataset from utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "816d56c1338745979dedd9bf253c68f0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "325437bede25433aaf9ff9fb78009cd2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/3038 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "80f3aa7b9a904ad1a8cbfcc3b2c2a8f0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ae092bd5234b47cabf6cc4103483b445"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "da5821c33b3c47dd8188ccd8ee68b55d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/3038 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "84461e6ae4984656946f1934edf38169"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset initialization complete.\n"
     ]
    }
   ],
   "source": [
    "cur_path = Path(\"plots_and_graphs.ipynb\")\n",
    "parent_dir = cur_path.parent.absolute()\n",
    "masterThesis_folder = str(parent_dir.parent.absolute())+'/'\n",
    "data_dir = masterThesis_folder+\"data/Clean_LiTS/\"\n",
    "\n",
    "cache_me = False\n",
    "if cache_me is True:\n",
    "    cache_mgr = Manager()\n",
    "    cache_mgr.data = cache_mgr.dict()\n",
    "    cache_mgr.cached = cache_mgr.dict()\n",
    "    for k in [\"train\", \"val\", \"test\"]:\n",
    "        cache_mgr.data[k] = cache_mgr.dict()\n",
    "        cache_mgr.cached[k] = False\n",
    "# function from utils, credit: Institute for Artificial Intelligence in Medicine. url: https://mml.ikim.nrw/\n",
    "# dataset outputs a tensor image (dimensions [1,256,256]) and a tensor target (0, 1 or 2)\n",
    "\n",
    "ds = uu.LiTS_Classification_Dataset(\n",
    "    data_dir=data_dir,\n",
    "    transforms=data_augments,\n",
    "    verbose=True,\n",
    "    cache_data=cache_me,\n",
    "    cache_mgr=(cache_mgr if cache_me is True else None),\n",
    "    debug=True,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default settings\n",
    "batch_size = 32\n",
    "learning_rate = 1e-4\n",
    "weight_decay = 5e-5\n",
    "epochs = 15\n",
    "run_name = \"AlexNet\"\n",
    "device = (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "time_me  = True"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "The `torch.utils.data.DataLoader` is a utility class in PyTorch that makes the loading and batching of data for training purposes faster. It simplifies the process by allowing us to specify the dataset, batch size (often 32), and whether the data should be shuffled before each epoch. Additionally, there are other parameters available to further customize the data loading process."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataloader\n",
    "dl = torch.utils.data.DataLoader(\n",
    "    dataset = ds, \n",
    "    batch_size = batch_size, \n",
    "    num_workers = 4, \n",
    "    shuffle = True, \n",
    "    drop_last = False, \n",
    "    pin_memory = True,\n",
    "    persistent_workers = (not cache_me),\n",
    "    prefetch_factor = 1\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# AlexNet\n",
    "\n",
    "AlexNet is a deep convolutional neural network architecture that was introduced in 2012 by Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton. It was one of the first successful models to use deep convolutional neural networks for image classification and won the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) in 2012. AlexNet consists of eight layers, including five convolutional layers and three fully connected layers, and uses ReLU activation functions and pooling layers to reduce the dimensionality of the input data.\n",
    "\n",
    "Some bullet points:\n",
    "1. ReLU activation function instead of tanh.\n",
    "2. Using dropout to reduce overfitting.\n",
    "3. Main idea: convolution followed by max pooling, and stacking of these layers; using dropout for fully connected layers.\n",
    "4. Using parallel computations on multiple GPUs (not included here).\n",
    "5. Local Response Normalization - not used in future networks; see torch.nn.LocalResponseNorm.\n",
    "\n",
    "Overall architecture and channel sizes can be found in [1]."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AlexNet Class\n",
    "#       - constructs a convolutional neuronal network as described in [1]\n",
    "# Input:    Tensor: [Batch,1,Height,Width]\n",
    "# Output:   Tensor: [Batch,3]\n",
    "class AlexNetMLMed(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AlexNetMLMed, self).__init__()\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.conv1 = torch.nn.Conv2d(in_channels = 1, out_channels = 96, kernel_size = (11,11), stride = (4,4), padding=1)        \n",
    "        self.pool1 = torch.nn.MaxPool2d(kernel_size = 3, stride = 2)\n",
    "        self.responseNorm = torch.nn.LocalResponseNorm(5)\n",
    "        self.conv2 = torch.nn.Conv2d(in_channels = 96, out_channels = 256, kernel_size = (5, 5), stride = (1,1), padding=1)\n",
    "        self.pool2 = torch.nn.MaxPool2d(kernel_size = 3, stride = 2)\n",
    "\n",
    "        self.conv3 = torch.nn.Conv2d(in_channels = 256, out_channels = 384, kernel_size = (3, 3), stride = (1,1), padding=1)\n",
    "        \n",
    "        self.conv4 = torch.nn.Conv2d(in_channels = 384, out_channels = 384, kernel_size = (3, 3), stride = (1,1), padding=1)\n",
    "\n",
    "        self.conv5 = torch.nn.Conv2d(in_channels = 384, out_channels = 256, kernel_size = (3, 3), stride = (1,1), padding=1)\n",
    "\n",
    "        self.pool4 = torch.nn.MaxPool2d(kernel_size = 3, stride = 2)\n",
    "        \n",
    "        self.dropout = torch.nn.Dropout(p=0.5) \n",
    "\n",
    "        self.fc1 = nn.Linear(6400,4096)\n",
    "        self.fc2 = nn.Linear(4096,4096)\n",
    "        self.fc3 = nn.Linear(4096,3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.relu(self.conv1(x))\n",
    "        out = self.pool1(out)\n",
    "        out = self.responseNorm(out)\n",
    "\n",
    "        out = self.relu(self.conv2(out))\n",
    "        out = self.pool2(out)\n",
    "        out = self.responseNorm(out)\n",
    "\n",
    "\n",
    "        out = self.relu(self.conv3(out))\n",
    "        out = self.relu(self.conv4(out))\n",
    "        out = self.relu(self.conv5(out))\n",
    "        out = self.pool4(out)\n",
    "\n",
    "        out = out.flatten(start_dim=1)\n",
    "\n",
    "        out = self.dropout(out)\n",
    "        out = self.relu(self.fc1(out))\n",
    "        out = self.dropout(out)\n",
    "        out = self.relu(self.fc2(out))\n",
    "        out = self.dropout(out)\n",
    "        out = self.relu(self.fc3(out))\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AlexNetMLMed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 96, 54, 54]          11,712\n",
      "         MaxPool2d-2           [-1, 96, 26, 26]               0\n",
      " LocalResponseNorm-3           [-1, 96, 26, 26]               0\n",
      "            Conv2d-4          [-1, 256, 24, 24]         614,656\n",
      "         MaxPool2d-5          [-1, 256, 11, 11]               0\n",
      " LocalResponseNorm-6          [-1, 256, 11, 11]               0\n",
      "            Conv2d-7          [-1, 384, 11, 11]         885,120\n",
      "            Conv2d-8          [-1, 384, 11, 11]       1,327,488\n",
      "            Conv2d-9          [-1, 256, 11, 11]         884,992\n",
      "        MaxPool2d-10            [-1, 256, 5, 5]               0\n",
      "          Dropout-11                 [-1, 6400]               0\n",
      "           Linear-12                 [-1, 4096]      26,218,496\n",
      "          Dropout-13                 [-1, 4096]               0\n",
      "           Linear-14                 [-1, 4096]      16,781,312\n",
      "          Dropout-15                 [-1, 4096]               0\n",
      "           Linear-16                    [-1, 3]          12,291\n",
      "================================================================\n",
      "Total params: 46,736,067\n",
      "Trainable params: 46,736,067\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.19\n",
      "Forward/backward pass size (MB): 5.89\n",
      "Params size (MB): 178.28\n",
      "Estimated Total Size (MB): 184.37\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(model, (1, 224, 224) )"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for step, (data, targets) in enumerate(dl):\n",
    "    data, targets = data.to(device), targets.to(device)\n",
    "    if step == 1:\n",
    "        break\n",
    "\n",
    "model = model.to(device)\n",
    "model(data).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate, weight_decay = weight_decay)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loop_conf_matr(\n",
    "    epochs = epochs,\n",
    "    optimizer = optimizer,\n",
    "    model = model,\n",
    "    criterion = criterion,\n",
    "    ds = ds,\n",
    "    dl = dl,\n",
    "    batch_size = batch_size,\n",
    "    run_name = run_name,\n",
    "    device = device,\n",
    "    time_me=True,\n",
    "    time=time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "--------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlcourse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 (main, Aug 25 2022, 23:26:10) \n[GCC 11.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5555b6f2b0077a97828fbbfe12cb97727895c9c472121c1b71224aa97370345d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
