{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "This notebook was motivated by\n",
    "\n",
    "[2] Kaiming He et al. ‘Deep Residual Learning for Image Recognition’. In: CoRR abs/1512.03385 (2015). arXiv: 1512.03385.\n",
    "url: http: //arxiv.org/abs/1512.03385.\n",
    "\n",
    "Implementation: Oleh Bakumenko, University of Duisburg-Essen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import torch, torch.nn as nn\n",
    "from torch.multiprocessing import Manager\n",
    "torch.multiprocessing.set_sharing_strategy(\"file_system\")\n",
    "import torchvision, torchvision.transforms as tt\n",
    "from torchsummary import summary\n",
    "from pathlib import Path\n",
    "\n",
    "from utility import utils as uu\n",
    "from utility.eval import evaluate_classifier_model\n",
    "from utility.confusion_matrix import calculate_confusion_matrix\n",
    "from utility.trainLoopClassifier import *\n",
    "from utility.plotImageModel import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data augmentations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Data augmentation is a technique used to artificially increase the size of a dataset by transforming existing data points to create new, similar instances. This can help prevent overfitting in machine learning models, as well as improve their ability to generalize to unseen data. Common types of data augmentation include flipping, rotation, scaling, and adding noise to images.\n",
    "We can generate the augmentation list with torchvision.transforms module\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augments = torchvision.transforms.Compose([ \n",
    "    torchvision.transforms.RandomHorizontalFlip(p = .5),\n",
    "    torchvision.transforms.RandomVerticalFlip(p = .5),\n",
    "    torchvision.transforms.ColorJitter(brightness=(0.5,1.5), contrast=(1), hue=(-0.1,0.1)),\n",
    "    #torchvision.transforms.RandomCrop((224, 224)),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Load the dataset from utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4c37c0dacc784719b5aae6a353763884"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/3 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e24af71e3e184c11a6bc660a8150a3bc"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/3038 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9c3c25c70d374d17b79bac8a8175a44f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d9476a126d2b492f9276f469bf8d7c1d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/3039 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "128022d20daa49fe8ab1980fdf48327d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/3038 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "db02856bb1f649b9a0187e5d70950bbc"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset initialization complete.\n"
     ]
    }
   ],
   "source": [
    "cur_path = Path(\"plots_and_graphs.ipynb\")\n",
    "parent_dir = cur_path.parent.absolute()\n",
    "masterThesis_folder = str(parent_dir.parent.absolute())+'/'\n",
    "data_dir = masterThesis_folder+\"data/Clean_LiTS/\"\n",
    "\n",
    "cache_me = False\n",
    "if cache_me is True:\n",
    "    cache_mgr = Manager()\n",
    "    cache_mgr.data = cache_mgr.dict()\n",
    "    cache_mgr.cached = cache_mgr.dict()\n",
    "    for k in [\"train\", \"val\", \"test\"]:\n",
    "        cache_mgr.data[k] = cache_mgr.dict()\n",
    "        cache_mgr.cached[k] = False\n",
    "# function from utils, credit: Institute for Artificial Intelligence in Medicine. url: https://mml.ikim.nrw/\n",
    "# dataset outputs a tensor image (dimensions [1,256,256]) and a tensor target (0, 1 or 2)\n",
    "\n",
    "ds = uu.LiTS_Classification_Dataset(\n",
    "    data_dir=data_dir,\n",
    "    transforms=data_augments,\n",
    "    verbose=True,\n",
    "    cache_data=cache_me,\n",
    "    cache_mgr=(cache_mgr if cache_me is True else None),\n",
    "    debug=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# Default settings\n",
    "batch_size = 32\n",
    "learning_rate = 1e-4\n",
    "weight_decay = 1e-5\n",
    "epochs = 15\n",
    "run_name = \"ResNet18\"\n",
    "device = (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "time_me  = True"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The `torch.utils.data.DataLoader` is a utility class in PyTorch that makes the loading and batching of data for training purposes faster. It simplifies the process by allowing us to specify the dataset, batch size (often 32), and whether the data should be shuffled before each epoch. Additionally, there are other parameters available to further customize the data loading process."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataloader\n",
    "dl = torch.utils.data.DataLoader(\n",
    "    dataset = ds, \n",
    "    batch_size = batch_size, \n",
    "    num_workers = 4, \n",
    "    shuffle = True, \n",
    "    drop_last = False, \n",
    "    pin_memory = True,\n",
    "    persistent_workers = (not cache_me),\n",
    "    prefetch_factor = 1\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "ResNet (Residual Network) is a deep neural network architecture introduced in 2015 in [2]. It was designed to address the issue of vanishing gradients in very deep networks. ResNet is named as such because it utilizes residual connections (skip connections), which enable the flow of gradients from earlier layers to later layers, even in very deep networks.\n",
    "\n",
    "The residual connections in ResNet involve adding the input of a layer to the output of a layer that is several layers deeper. This allows the network to more easily learn identity functions. This design helps prevent the issue of vanishing gradients and enables ResNet to train much deeper networks than was previously possible. This architecture has shown significant improvements in benchmarks compared to the earlier AlexNet model.\n",
    "\n",
    "The original ResNet was used in the ImageNet Challenge to classify 1000 classes. However, in our exercise, we only use 3 classes:\n",
    "0: Image does not include the liver.\n",
    "1: Liver is visible.\n",
    "2: Liver is visible and a lesion is visible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# ResNet 18"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We highly recommend cross-referencing Table 1 on page 5 and Figure 5 on page 6 of reference [2] simultaneously.\n",
    "\n",
    "To implement the normal ResNet Block, we use the following sequence: [conv -> batch_norm -> activation] * 2.\n",
    "\n",
    "At the beginning of each new layer (as shown in Table 1, left), the image size is reduced using convolution with a kernel size of 1 and a stride of 2 (known as projection). This feature was generalized in the implementation of ResNet 50. As an example, we have decided to include both variations.\n",
    "\n",
    "First, we start by building the blocks. Please note the downsampling operation in the ResBlockDimsReduction, as the input image $x$ has different dimensions than the output.\n",
    "\n",
    "The class ResNetMLMed18 will inherit from torch.nn.Module, so we need to implement the init() and forward() functions. Using Table 1 and Figure 5 from [2], we define each part of resblocks2-5. The indexing follows the same convention as in Table 1, allowing for easy comparison of block numbers, kernel sizes, and number of channels.\n",
    "\n",
    "The DimsReduction block is the first block in resblocks2-5, as it performs downsampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "A few words about the torch.nn.init part:\n",
    "PyTorch initializes the parameters for Conv and batch norm randomly from uniform distribution. Initialization of the weights and biases with a normal distribution helps the model backpropagate gradients in early epochs.\n",
    "\n",
    "Tests were conducted on smaller models with 18, 34, and 50 layers, indicating that for adaptive optimizers, weight and bias initialization has minimal effect on model performance or convergence.\n",
    "\n",
    "In contrast, the uniform initialized ResNet 152 model exhibited poor convergence after 15 epochs, with very high error and low accuracy rates. Although initialization improved the performance, it still required tuning of hyperparameters and a better optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ResBlock Class\n",
    "#       - constructs a block [conv -> batch_norm -> activation] *2, which we will stack in the network\n",
    "# Input:    int: n_chans - number channels\n",
    "# Output:   nn.Sequential() block\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, n_chans, init_weights=True ):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(n_chans, n_chans, kernel_size=3, padding=1, bias= False)\n",
    "        self.batch_norm1 = nn.BatchNorm2d(num_features=n_chans)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.conv2 = nn.Conv2d(n_chans, n_chans, kernel_size=3, padding=1, bias= False)\n",
    "        self.batch_norm2 = nn.BatchNorm2d(num_features=n_chans)\n",
    "\n",
    "        torch.nn.init.kaiming_normal_(self.conv1.weight,nonlinearity='relu')\n",
    "        torch.nn.init.kaiming_normal_(self.conv2.weight,nonlinearity='relu')\n",
    "        torch.nn.init.constant_(self.batch_norm1.weight, 0.5)\n",
    "        torch.nn.init.zeros_(self.batch_norm1.bias)\n",
    "        torch.nn.init.constant_(self.batch_norm2.weight, 0.5)\n",
    "        torch.nn.init.zeros_(self.batch_norm2.bias)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.batch_norm1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.batch_norm2(out)\n",
    "        out = self.relu(out)\n",
    "        return out + x # the skip connection\n",
    "\n",
    "\n",
    "# ResBlockDimsReduction Class\n",
    "#       - constructs a first block in the layer\n",
    "#       - [conv -> batch_norm -> activation] *2\n",
    "#       - downsampling performed with stride 2\n",
    "# Input:    int: num_chans_in; int:num_chans_out\n",
    "# Output:   nn.Sequential() block\n",
    "class ResBlockDimsReduction(nn.Module):\n",
    "    def __init__(self, n_chans_in, n_chans_out):\n",
    "        super(ResBlockDimsReduction, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(n_chans_in, n_chans_out, kernel_size=3, stride=2,padding=1)\n",
    "        self.batch_norm1 = nn.BatchNorm2d(num_features=n_chans_out)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.conv2 = nn.Conv2d(n_chans_out, n_chans_out, kernel_size=3,padding=1)\n",
    "        self.batch_norm2 = nn.BatchNorm2d(num_features=n_chans_out)\n",
    "\n",
    "        torch.nn.init.kaiming_normal_(self.conv1.weight,nonlinearity='relu')\n",
    "        torch.nn.init.kaiming_normal_(self.conv2.weight, nonlinearity='relu')\n",
    "        torch.nn.init.constant_(self.batch_norm1.weight, 0.5)\n",
    "        torch.nn.init.zeros_(self.batch_norm1.bias)\n",
    "        torch.nn.init.constant_(self.batch_norm2.weight, 0.5)\n",
    "        torch.nn.init.zeros_(self.batch_norm2.bias)\n",
    "\n",
    "        self.downsample = nn.Sequential(\n",
    "            nn.Conv2d(n_chans_in, n_chans_out, kernel_size=1, stride=2),\n",
    "            nn.BatchNorm2d(num_features=n_chans_out)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.batch_norm1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.batch_norm2(out)\n",
    "        if (x.shape[1] != out.shape[1]): # input and output dimensions not match, so we need to project x into the dimensions of out\n",
    "            x = self.downsample(x)\n",
    "        return out + x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# ResNetMLMed18 Class\n",
    "#       - constructs a ResNet34 as described in [2, Table 1].\n",
    "# Input:    Tensor: [Batch,1,Height,Width]\n",
    "# Output:   Tensor: [Batch,3]\n",
    "class ResNetMLMed18(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = torch.nn.Conv2d(in_channels = 1, out_channels = 64, kernel_size =7, stride =2, padding=1)\n",
    "        self.batch_norm1 = nn.BatchNorm2d(num_features=64)\n",
    "        self.pool2 = torch.nn.MaxPool2d(kernel_size = 3, stride = 2)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "\n",
    "        self.resblocks2 =nn.Sequential(\n",
    "            *(2 * [ResBlock(n_chans=64)]))\n",
    "        self.resblocks3 = nn.Sequential(ResBlockDimsReduction(n_chans_in=64,n_chans_out=128),\n",
    "            *(1 * [ResBlock(n_chans=128)]))\n",
    "        self.resblocks4 = nn.Sequential(ResBlockDimsReduction(n_chans_in=128,n_chans_out=256),\n",
    "            *(1 * [ResBlock(n_chans=256)]))\n",
    "        self.resblocks5 = nn.Sequential(ResBlockDimsReduction(n_chans_in=256,n_chans_out=512),\n",
    "            *(1 * [ResBlock(n_chans=512)]))\n",
    "        self.avgpool6 = nn.AdaptiveAvgPool2d(output_size=(1, 1))\n",
    "        self.fc = nn.Linear(in_features=512, out_features=3, bias=True)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        out_1 = self.conv1(x)\n",
    "        out_1 = self.batch_norm1(out_1)\n",
    "        out_1 = self.relu(out_1)\n",
    "        out_1 = self.pool2(out_1)\n",
    "\n",
    "        out_2 = self.resblocks2(out_1)\n",
    "\n",
    "        out_3 = self.resblocks3(out_2)\n",
    "\n",
    "        out_4 = self.resblocks4(out_3)\n",
    "\n",
    "        out_5 = self.resblocks5(out_4)\n",
    "\n",
    "        out_6 = self.avgpool6(out_5)\n",
    "\n",
    "        out_6= self.fc(torch.flatten(out_6, start_dim=1))\n",
    "\n",
    "        return out_6"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 126, 126]           3,200\n",
      "       BatchNorm2d-2         [-1, 64, 126, 126]             128\n",
      "              ReLU-3         [-1, 64, 126, 126]               0\n",
      "         MaxPool2d-4           [-1, 64, 62, 62]               0\n",
      "            Conv2d-5           [-1, 64, 62, 62]          36,864\n",
      "       BatchNorm2d-6           [-1, 64, 62, 62]             128\n",
      "              ReLU-7           [-1, 64, 62, 62]               0\n",
      "            Conv2d-8           [-1, 64, 62, 62]          36,864\n",
      "       BatchNorm2d-9           [-1, 64, 62, 62]             128\n",
      "             ReLU-10           [-1, 64, 62, 62]               0\n",
      "         ResBlock-11           [-1, 64, 62, 62]               0\n",
      "           Conv2d-12           [-1, 64, 62, 62]          36,864\n",
      "      BatchNorm2d-13           [-1, 64, 62, 62]             128\n",
      "             ReLU-14           [-1, 64, 62, 62]               0\n",
      "           Conv2d-15           [-1, 64, 62, 62]          36,864\n",
      "      BatchNorm2d-16           [-1, 64, 62, 62]             128\n",
      "             ReLU-17           [-1, 64, 62, 62]               0\n",
      "         ResBlock-18           [-1, 64, 62, 62]               0\n",
      "           Conv2d-19          [-1, 128, 31, 31]          73,856\n",
      "      BatchNorm2d-20          [-1, 128, 31, 31]             256\n",
      "             ReLU-21          [-1, 128, 31, 31]               0\n",
      "           Conv2d-22          [-1, 128, 31, 31]         147,584\n",
      "      BatchNorm2d-23          [-1, 128, 31, 31]             256\n",
      "           Conv2d-24          [-1, 128, 31, 31]           8,320\n",
      "      BatchNorm2d-25          [-1, 128, 31, 31]             256\n",
      "ResBlockDimsReduction-26          [-1, 128, 31, 31]               0\n",
      "           Conv2d-27          [-1, 128, 31, 31]         147,456\n",
      "      BatchNorm2d-28          [-1, 128, 31, 31]             256\n",
      "             ReLU-29          [-1, 128, 31, 31]               0\n",
      "           Conv2d-30          [-1, 128, 31, 31]         147,456\n",
      "      BatchNorm2d-31          [-1, 128, 31, 31]             256\n",
      "             ReLU-32          [-1, 128, 31, 31]               0\n",
      "         ResBlock-33          [-1, 128, 31, 31]               0\n",
      "           Conv2d-34          [-1, 256, 16, 16]         295,168\n",
      "      BatchNorm2d-35          [-1, 256, 16, 16]             512\n",
      "             ReLU-36          [-1, 256, 16, 16]               0\n",
      "           Conv2d-37          [-1, 256, 16, 16]         590,080\n",
      "      BatchNorm2d-38          [-1, 256, 16, 16]             512\n",
      "           Conv2d-39          [-1, 256, 16, 16]          33,024\n",
      "      BatchNorm2d-40          [-1, 256, 16, 16]             512\n",
      "ResBlockDimsReduction-41          [-1, 256, 16, 16]               0\n",
      "           Conv2d-42          [-1, 256, 16, 16]         589,824\n",
      "      BatchNorm2d-43          [-1, 256, 16, 16]             512\n",
      "             ReLU-44          [-1, 256, 16, 16]               0\n",
      "           Conv2d-45          [-1, 256, 16, 16]         589,824\n",
      "      BatchNorm2d-46          [-1, 256, 16, 16]             512\n",
      "             ReLU-47          [-1, 256, 16, 16]               0\n",
      "         ResBlock-48          [-1, 256, 16, 16]               0\n",
      "           Conv2d-49            [-1, 512, 8, 8]       1,180,160\n",
      "      BatchNorm2d-50            [-1, 512, 8, 8]           1,024\n",
      "             ReLU-51            [-1, 512, 8, 8]               0\n",
      "           Conv2d-52            [-1, 512, 8, 8]       2,359,808\n",
      "      BatchNorm2d-53            [-1, 512, 8, 8]           1,024\n",
      "           Conv2d-54            [-1, 512, 8, 8]         131,584\n",
      "      BatchNorm2d-55            [-1, 512, 8, 8]           1,024\n",
      "ResBlockDimsReduction-56            [-1, 512, 8, 8]               0\n",
      "           Conv2d-57            [-1, 512, 8, 8]       2,359,296\n",
      "      BatchNorm2d-58            [-1, 512, 8, 8]           1,024\n",
      "             ReLU-59            [-1, 512, 8, 8]               0\n",
      "           Conv2d-60            [-1, 512, 8, 8]       2,359,296\n",
      "      BatchNorm2d-61            [-1, 512, 8, 8]           1,024\n",
      "             ReLU-62            [-1, 512, 8, 8]               0\n",
      "         ResBlock-63            [-1, 512, 8, 8]               0\n",
      "AdaptiveAvgPool2d-64            [-1, 512, 1, 1]               0\n",
      "           Linear-65                    [-1, 3]           1,539\n",
      "================================================================\n",
      "Total params: 11,174,531\n",
      "Trainable params: 11,174,531\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.25\n",
      "Forward/backward pass size (MB): 76.74\n",
      "Params size (MB): 42.63\n",
      "Estimated Total Size (MB): 119.62\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = ResNetMLMed18()\n",
    "summary(model, (1, 256, 256))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([2, 3])"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for step, (data, targets) in enumerate(dl):\n",
    "    data, targets = data.to(device), targets.to(device)\n",
    "    if step ==1:\n",
    "        break\n",
    "\n",
    "model = model.to(device)\n",
    "model(data).shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate, weight_decay = weight_decay)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loop_conf_matr(\n",
    "    epochs = epochs,\n",
    "    optimizer = optimizer,\n",
    "    model = model,\n",
    "    criterion = criterion,\n",
    "    ds = ds,\n",
    "    dl = dl,\n",
    "    batch_size = batch_size,\n",
    "    run_name = run_name,\n",
    "    device = device,\n",
    "    time_me=True,\n",
    "    time=time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlcourse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5555b6f2b0077a97828fbbfe12cb97727895c9c472121c1b71224aa97370345d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
