{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "This notebook was motivated by\n",
    "\n",
    "[4] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. “U-Net: Convolutional Networks for Biomedical Image Segmentation”.\n",
    "In: CoRRabs/1505.04597 (2015). arXiv: 1505.04597. url: http://arxiv.org/abs/1505.04597\n",
    "\n",
    "Implementation: Oleh Bakumenko, University of Duisburg-Essen"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\") # Otherwise, import from the local folder's parent folder, where your stuff lives.\n",
    "import PIL\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import time\n",
    "import torch, torch.nn as nn\n",
    "import albumentations\n",
    "from typing import List\n",
    "from torch.multiprocessing import Manager\n",
    "torch.multiprocessing.set_sharing_strategy(\"file_system\")\n",
    "\n",
    "from utility import utils as uu\n",
    "from utility.eval import evaluate_segmentation_model\n",
    "from utility.segloss import ExampleSegmentationLoss\n",
    "from pathlib import Path\n",
    "\n",
    "from utility.plotImageModel import *\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data augmentations"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Albumentations is a Python library for image augmentation, which apply augmentations also on the target. Some of the commonly used transformations include:\n",
    "random cropping, random flips of the image horizontally or vertically, rotating the image by a certain angle, rescaleing of the image by a given factor or resizes it to a specific size, adjusting the brightness and contrast, appling Gaussian blur and so on.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_path = Path(\"plots_and_graphs.ipynb\")\n",
    "parent_dir = cur_path.parent.absolute()\n",
    "masterThesis_folder = str(parent_dir.parent.absolute())+'/'\n",
    "data_dir = masterThesis_folder+\"data/Clean_LiTS/\"\n",
    "\n",
    "augments = albumentations.Compose([\n",
    "    albumentations.VerticalFlip(p=0.5),\n",
    "    albumentations.HorizontalFlip(p=0.5),\n",
    "    albumentations.ColorJitter(brightness=(0.5,1.5), contrast=(1), hue=(-0.1,0.1)),\n",
    "])\n",
    "\n",
    "data_augments = augments\n",
    "\n",
    "# Train, Val, and Test datasets are all contained within this dataset.\n",
    "# They can be selected by setting 'ds.set_mode(selection)'.\n",
    "\n",
    "# We could also cache any data we read from disk to shared memory, or\n",
    "# to regular memory, where each dataloader worker caches the entire\n",
    "# dataset.\n",
    "\n",
    "cache_me = False\n",
    "if cache_me is True:\n",
    "    cache_mgr = Manager()\n",
    "    cache_mgr.data = cache_mgr.dict()\n",
    "    cache_mgr.cached = cache_mgr.dict()\n",
    "    for k in [\"train\", \"val\", \"test\"]:\n",
    "        cache_mgr.data[k] = cache_mgr.dict()\n",
    "        cache_mgr.cached[k] = False\n",
    "\n",
    "# function from utils, credit: Institute for Artificial Intelligence in Medicine.\n",
    "# url: https://mml.ikim.nrw/\n",
    "\n",
    "# dataset outputs a tensor image (dimensions [1,256,256]) and a target list\n",
    "ds = uu.LiTS_Segmentation_Dataset(\n",
    "    data_dir = data_dir,\n",
    "    transforms = data_augments,\n",
    "    verbose = True,\n",
    "    cache_data = cache_me,\n",
    "    cache_mgr = (cache_mgr if cache_me is True else None),\n",
    "    debug = True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default settings\n",
    "batch_size = 16\n",
    "learning_rate = 1e-4\n",
    "weight_decay = 1e-5\n",
    "epochs = 6\n",
    "run_name = \"UNet\"\n",
    "device = (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "time_me = True\n",
    "mod_step=500"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "The `torch.utils.data.DataLoader` is a utility class in PyTorch that makes the loading and batching of data for training purposes faster. It simplifies the process by allowing us to specify the dataset, batch size (often 32), and whether the data should be shuffled before each epoch. Additionally, there are other parameters available to further customize the data loading process."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataloader\n",
    "dl = torch.utils.data.DataLoader(\n",
    "    dataset = ds, \n",
    "    batch_size = batch_size, \n",
    "    num_workers = 4, \n",
    "    shuffle = True, \n",
    "    drop_last = False, \n",
    "    pin_memory = True,\n",
    "    persistent_workers = (not cache_me),\n",
    "    prefetch_factor = 1\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# U-Net"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The U-Net model is primarily designed for performing pixel-level or region-level classification within an input image. Its main objective is to generate a segmentation map where each pixel in the input image is assigned a label indicating its class. The input dimensions  are (B x 1 x 256 x 256) and output dimensions  are (B x 3 x 256 x 256).\n",
    "\n",
    "The main features of the U-Net architecture are:\n",
    "\n",
    "1. Encoder: The encoder path of the U-Net consists of multiple down-sampling layers. Each down-sampling layer includes two convolutional layers followed by a max-pooling operation. These layers progressively reduce the spatial dimensions of the input image while increasing the number of channels. This allows the model to capture local information and extract lower-level features.\n",
    "\n",
    "2. Decoder: The decoder path is a mirrored version of the encoder path. It comprises up-sampling layers followed by convolutional layers. The up-sampling layers utilize transposed (inverse) convolution to increase the spatial dimensions of the feature maps. The decoder path plays an important role in recovering the spatial information lost during the down-sampling process and reconstructing the segmented image.\n",
    "\n",
    "3. Skip Connections: The U-Net architecture incorporates skip connections between the encoder and decoder paths. These connections enable the model to merge (concate) feature maps from the encoder path with corresponding feature maps from the decoder path at the same scale. By fusing high-resolution features from the encoder with up-sampled features from the decoder, the U-Net effectively combines both local and global context information, resulting in accurate segmentation outcomes.\n",
    "\n",
    "Our modifications to the model include the following:\n",
    "\n",
    "1. Horizontal 3x3 convolutions will have padding to avoid cropping during skip connections.\n",
    "2. There will be 3 downsample steps and corresponding skip connections, rather than 4. Therefore, the maximum feature size in the bottom layer will be 512.\n",
    "3. The final output will be 3 channels wide, as we predict background, liver, and liver tumors."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "For each horizontal convolution, the generalized double convolution block is employed. This block comprises two convolutions with a 3x3 kernel, batch normalization to tackle overfitting, and ReLU activation. Furthermore, weight initialization, which has proven effective in classification networks, is also included into the block."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Conv_Block(nn.Module):\n",
    "    def __init__(self, n_chans_in,n_chans_out):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(n_chans_in, n_chans_out, kernel_size=3, padding=1,bias= False)\n",
    "        self.batch_norm1 = nn.BatchNorm2d(num_features=n_chans_out)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.conv2 = nn.Conv2d(n_chans_out, n_chans_out, kernel_size=3, padding=1,bias= False)\n",
    "        self.batch_norm2 = nn.BatchNorm2d(num_features=n_chans_out)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "\n",
    "        torch.nn.init.kaiming_normal_(self.conv1.weight,nonlinearity='relu')\n",
    "        torch.nn.init.kaiming_normal_(self.conv2.weight,nonlinearity='relu')\n",
    "        torch.nn.init.constant_(self.batch_norm1.weight, 0.5)\n",
    "        torch.nn.init.zeros_(self.batch_norm1.bias)\n",
    "        torch.nn.init.constant_(self.batch_norm2.weight, 0.5)\n",
    "        torch.nn.init.zeros_(self.batch_norm2.bias)\n",
    "    def forward(self, x):\n",
    "        out1 = self.relu(self.batch_norm1(self.conv1(x)))\n",
    "        out  = self.relu(self.batch_norm2(self.conv2(out1)))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class UNetMLMed(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxPool = nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "        self.encoder_layer1 = Conv_Block(1,64)\n",
    "        self.crop1 = nn.Identity()\n",
    "        self.encoder_layer2 = Conv_Block(64,128)\n",
    "        self.crop2 = nn.Identity()\n",
    "        self.encoder_layer3 = Conv_Block(128,256)\n",
    "        self.crop3 = nn.Identity()\n",
    "        self.encoder_layer4 = Conv_Block(256,512)\n",
    "\n",
    "        self.decoder_upwards4 = torch.nn.ConvTranspose2d(in_channels=512,out_channels=256,kernel_size=2,stride=2)\n",
    "        self.decoder_layer3 = Conv_Block(512,256)\n",
    "\n",
    "        self.decoder_upwards3 = torch.nn.ConvTranspose2d(in_channels=256,out_channels=128,kernel_size=2,stride=2)\n",
    "        self.decoder_layer2 = Conv_Block(256,128)\n",
    "\n",
    "        self.decoder_upwards2 = torch.nn.ConvTranspose2d(in_channels=128,out_channels=64 ,kernel_size=2,stride=2)\n",
    "        self.decoder_layer1 = Conv_Block(128,64)\n",
    "\n",
    "        self.output_segmentation = nn.Conv2d(in_channels=64, out_channels= 3, kernel_size=1)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # a list to save the skip connections\n",
    "        save = list()\n",
    "        # encoder part = [Conv_Block() + MaxPool()] x3\n",
    "        encoder_out1 = self.encoder_layer1(x)\n",
    "        \n",
    "        save.append(self.crop1(encoder_out1))\n",
    "        encoder_in2 = self.maxPool(encoder_out1)\n",
    "        \n",
    "\n",
    "        encoder_out2 = self.encoder_layer2(encoder_in2)\n",
    "        save.append(self.crop2(encoder_out2))\n",
    "        encoder_in3 = self.maxPool(encoder_out2)\n",
    "        \n",
    "        encoder_out3 = self.encoder_layer3(encoder_in3)\n",
    "        save.append(self.crop3(encoder_out3))\n",
    "        encoder_in4 = self.maxPool(encoder_out3)\n",
    "        \n",
    "        encoder_out4 = self.encoder_layer4(encoder_in4)\n",
    "        upwards4 = self.decoder_upwards4(encoder_out4)\n",
    "        # the bottom of the network\n",
    "        decoder3_in = torch.concat([save[-1],upwards4], dim=1) # skip connection\n",
    "        # decoder part = [ TransposedConv() + Conv_Block()] x3\n",
    "        decoder3_out = self.decoder_layer3(decoder3_in)\n",
    "        \n",
    "        upwards3 = self.decoder_upwards3(decoder3_out)\n",
    "        decoder2_in = torch.concat([save[-2],upwards3], dim=1)# skip connection\n",
    "        decoder2_out = self.decoder_layer2(decoder2_in)\n",
    "        \n",
    "        upwards2 = self.decoder_upwards2(decoder2_out)\n",
    "        decoder1_in = torch.concat([save[-3],upwards2], dim=1)# skip connection\n",
    "        decoder1_out = self.decoder_layer1(decoder1_in)\n",
    "\n",
    "        output = self.output_segmentation(decoder1_out)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 256, 256]             576\n",
      "       BatchNorm2d-2         [-1, 64, 256, 256]             128\n",
      "              ReLU-3         [-1, 64, 256, 256]               0\n",
      "            Conv2d-4         [-1, 64, 256, 256]          36,864\n",
      "       BatchNorm2d-5         [-1, 64, 256, 256]             128\n",
      "              ReLU-6         [-1, 64, 256, 256]               0\n",
      "        Conv_Block-7         [-1, 64, 256, 256]               0\n",
      "          Identity-8         [-1, 64, 256, 256]               0\n",
      "         MaxPool2d-9         [-1, 64, 128, 128]               0\n",
      "           Conv2d-10        [-1, 128, 128, 128]          73,728\n",
      "      BatchNorm2d-11        [-1, 128, 128, 128]             256\n",
      "             ReLU-12        [-1, 128, 128, 128]               0\n",
      "           Conv2d-13        [-1, 128, 128, 128]         147,456\n",
      "      BatchNorm2d-14        [-1, 128, 128, 128]             256\n",
      "             ReLU-15        [-1, 128, 128, 128]               0\n",
      "       Conv_Block-16        [-1, 128, 128, 128]               0\n",
      "         Identity-17        [-1, 128, 128, 128]               0\n",
      "        MaxPool2d-18          [-1, 128, 64, 64]               0\n",
      "           Conv2d-19          [-1, 256, 64, 64]         294,912\n",
      "      BatchNorm2d-20          [-1, 256, 64, 64]             512\n",
      "             ReLU-21          [-1, 256, 64, 64]               0\n",
      "           Conv2d-22          [-1, 256, 64, 64]         589,824\n",
      "      BatchNorm2d-23          [-1, 256, 64, 64]             512\n",
      "             ReLU-24          [-1, 256, 64, 64]               0\n",
      "       Conv_Block-25          [-1, 256, 64, 64]               0\n",
      "         Identity-26          [-1, 256, 64, 64]               0\n",
      "        MaxPool2d-27          [-1, 256, 32, 32]               0\n",
      "           Conv2d-28          [-1, 512, 32, 32]       1,179,648\n",
      "      BatchNorm2d-29          [-1, 512, 32, 32]           1,024\n",
      "             ReLU-30          [-1, 512, 32, 32]               0\n",
      "           Conv2d-31          [-1, 512, 32, 32]       2,359,296\n",
      "      BatchNorm2d-32          [-1, 512, 32, 32]           1,024\n",
      "             ReLU-33          [-1, 512, 32, 32]               0\n",
      "       Conv_Block-34          [-1, 512, 32, 32]               0\n",
      "  ConvTranspose2d-35          [-1, 256, 64, 64]         524,544\n",
      "           Conv2d-36          [-1, 256, 64, 64]       1,179,648\n",
      "      BatchNorm2d-37          [-1, 256, 64, 64]             512\n",
      "             ReLU-38          [-1, 256, 64, 64]               0\n",
      "           Conv2d-39          [-1, 256, 64, 64]         589,824\n",
      "      BatchNorm2d-40          [-1, 256, 64, 64]             512\n",
      "             ReLU-41          [-1, 256, 64, 64]               0\n",
      "       Conv_Block-42          [-1, 256, 64, 64]               0\n",
      "  ConvTranspose2d-43        [-1, 128, 128, 128]         131,200\n",
      "           Conv2d-44        [-1, 128, 128, 128]         294,912\n",
      "      BatchNorm2d-45        [-1, 128, 128, 128]             256\n",
      "             ReLU-46        [-1, 128, 128, 128]               0\n",
      "           Conv2d-47        [-1, 128, 128, 128]         147,456\n",
      "      BatchNorm2d-48        [-1, 128, 128, 128]             256\n",
      "             ReLU-49        [-1, 128, 128, 128]               0\n",
      "       Conv_Block-50        [-1, 128, 128, 128]               0\n",
      "  ConvTranspose2d-51         [-1, 64, 256, 256]          32,832\n",
      "           Conv2d-52         [-1, 64, 256, 256]          73,728\n",
      "      BatchNorm2d-53         [-1, 64, 256, 256]             128\n",
      "             ReLU-54         [-1, 64, 256, 256]               0\n",
      "           Conv2d-55         [-1, 64, 256, 256]          36,864\n",
      "      BatchNorm2d-56         [-1, 64, 256, 256]             128\n",
      "             ReLU-57         [-1, 64, 256, 256]               0\n",
      "       Conv_Block-58         [-1, 64, 256, 256]               0\n",
      "           Conv2d-59          [-1, 3, 256, 256]             195\n",
      "================================================================\n",
      "Total params: 7,699,139\n",
      "Trainable params: 7,699,139\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.25\n",
      "Forward/backward pass size (MB): 939.50\n",
      "Params size (MB): 29.37\n",
      "Estimated Total Size (MB): 969.12\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = UNetMLMed()\n",
    "summary(UNetMLMed(), (1, 256, 256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate, weight_decay = weight_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Specific loss function."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropyPixelWiseLoss(torch.nn.Module):\n",
    "    \"\"\"\n",
    "        Computes  pixel wise Cross-Entropy Loss function\n",
    "        Inputs:\n",
    "            prediction: Torch tensor size: torch.Size([batch, 3, height, width])\n",
    "            targets: List of Torch tensors of length 2, each tensor is size: torch.Size([batch, 1, height, width])\n",
    "        Outputs:\n",
    "            pw_loss  = for each target sum_allPixels (-1)target_i*log(pred_i)\n",
    "    \"\"\"\n",
    "    def __init__(self, classes: int = 3, w_l: torch.Tensor = None):\n",
    "        super().__init__()\n",
    "        self.classes = classes\n",
    "        if w_l is None:\n",
    "            w_l = torch.Tensor([1 for c in range(self.classes)])\n",
    "        self.weights = w_l\n",
    "\n",
    "\n",
    "    def forward(self, predictions: torch.Tensor, targets: List[torch.Tensor,]):\n",
    "        # Predictions size: torch.Size([batch, 3, 256, 256]) \n",
    "        # targets: List of tensors of length 2\n",
    "        # each tensor is size: torch.Size([batch, 1, 256, 256])\n",
    " \n",
    "        batch = predictions.shape[0]\n",
    "        size = predictions.shape[2]\n",
    "        ones_matr = torch.ones(batch,1,size,size).to(device)\n",
    "        \n",
    "        softmax = nn.Softmax(dim=1)\n",
    "        predictions = softmax(predictions)\n",
    "\n",
    "        target_liver = targets[0]\n",
    "        target_cancer =  targets[1]\n",
    "        target_bg = ones_matr - target_liver\n",
    "\n",
    "        product_bg =       ((-1)*target_bg.squeeze()*torch.log(predictions[:, 0, :, :])).sum()\n",
    "        product_liver =    ((-1)*target_liver.squeeze()*torch.log(predictions[:, 1, :, :])).sum()\n",
    "        product_cancer =   ((-1)*target_cancer.squeeze()*torch.log(predictions[:, 2, :, :])).sum()\n",
    "\n",
    "        pw_loss = (self.weights[0]*product_bg+self.weights[1]*product_liver+self.weights[2]*product_cancer)/(batch*size*size)\n",
    "\n",
    "        return pw_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = CrossEntropyPixelWiseLoss(w_l = torch.Tensor([1, 3, 10])).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if time_me is True:\n",
    "    c_start = time.time()\n",
    "\n",
    "num_steps = len(ds.file_names['train'])//batch_size\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    # If we are caching, we now have all data and let the (potentially non-persistent) workers know\n",
    "    if cache_me is True and epoch > 0:\n",
    "        dl.dataset.set_cached(\"train\")\n",
    "        dl.dataset.set_cached(\"val\")\n",
    "    \n",
    "    # Time me\n",
    "    if time_me is True:\n",
    "        e_start = time.time()\n",
    "\n",
    "    # Go to train mode\n",
    "    ds.set_mode(\"train\")\n",
    "    model.train()\n",
    "\n",
    "    # Train loop\n",
    "    for step, (data, targets) in enumerate(dl):\n",
    "\n",
    "        # Manually drop last batch (this is for example relevant with BatchNorm)\n",
    "        if step == num_steps - 1 and (epoch > 0 or ds.cache_data is False):\n",
    "            continue\n",
    "\n",
    "        # Train loop: Zero gradients, forward step, evaluate, log, backward step\n",
    "        optimizer.zero_grad()\n",
    "        data = data.to(device)\n",
    "        targets = [target.to(device) for target in targets]\n",
    "        if time_me is True:\n",
    "            c_end = time.time()\n",
    "            if step % 20 == 0:\n",
    "                print(f\"CPU time: {c_end-c_start:.4f}s\")\n",
    "            g_start = time.time()\n",
    "        predictions = model(data)\n",
    "        if time_me is True:\n",
    "            g_end = time.time()\n",
    "            c_start = time.time()\n",
    "        if step % 20 == 0 and time_me is True:\n",
    "            print(f\"GPU time: {g_end-g_start:.4f}s\")        \n",
    "        loss = criterion(predictions, targets)\n",
    "        if step % mod_step == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{epochs}]\\t Step [{step+1}/{num_steps}]\\t Train Loss: {loss.item():.4f}\")\n",
    "        uu.csv_logger(\n",
    "            logfile = f\"../logs/{run_name}_train.csv\",\n",
    "            content = {\"epoch\": epoch, \"step\": step, \"loss\": loss.item()},\n",
    "            first = (epoch == 0 and step == 0),\n",
    "            overwrite = (epoch == 0 and step == 0)\n",
    "                )\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Go to eval mode\n",
    "    ds.set_mode(\"val\")\n",
    "    model.eval()\n",
    "\n",
    "    # Validation loop\n",
    "    metrics = {\"epoch\": epoch}\n",
    "    metrics.update(evaluate_segmentation_model(model = model, dataloader = dl, device = device))\n",
    "    print('\\n'.join([f'{m}: {v}' for m, v in metrics.items() if not m.startswith(\"#\")]))\n",
    "    uu.csv_logger(\n",
    "        logfile = f\"../logs/{run_name}_val.csv\",\n",
    "        content = {m: v for m, v in metrics.items() if not m.startswith(\"#\")},\n",
    "        first = (epoch == 0),\n",
    "        overwrite = (epoch == 0)\n",
    "            )\n",
    "        \n",
    "    if time_me is True:\n",
    "        cur_time = time.time()-e_start        \n",
    "        uu.csv_logger(\n",
    "            logfile = f\"../logs/{run_name}_runtime.csv\",\n",
    "            content = {\"epoch\": epoch, \"time\": cur_time},\n",
    "            first = (epoch == 0),\n",
    "            overwrite = (epoch == 0)\n",
    "                )\n",
    "        print(f\"Epoch nr {epoch+1} time: {time.time()-e_start:.4f}s\")\n",
    "\n",
    "# Finally, test time\n",
    "ds.set_mode(\"test\")\n",
    "model.eval()\n",
    "metrics = evaluate_segmentation_model(model = model, dataloader = dl, device = device)\n",
    "print(\"Test-time metrics:\")\n",
    "print('\\n'.join([f'{m}: {v}' for m, v in metrics.items() if not m.startswith(\"#\")]))\n",
    "uu.csv_logger(\n",
    "    logfile = f\"../logs/{run_name}_test.csv\",\n",
    "    content = {m: v for m, v in metrics.items() if not m.startswith(\"#\")},\n",
    "    first = True,\n",
    "    overwrite = True\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlcourse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 (main, Aug 25 2022, 23:26:10) \n[GCC 11.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5555b6f2b0077a97828fbbfe12cb97727895c9c472121c1b71224aa97370345d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
