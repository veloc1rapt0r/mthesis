{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "This notebook was motivated by\n",
    "\n",
    "https://arxiv.org/pdf/1512.03385.pdf\n",
    "\n",
    "will be later cited as ResNetPaper\n",
    "Implementation: Oleh Bakumenko, University of Duisburg-Essen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import torch, torch.nn as nn\n",
    "import torchvision, torchvision.transforms as tt\n",
    "from torch.multiprocessing import Manager\n",
    "torch.multiprocessing.set_sharing_strategy(\"file_system\")\n",
    "from pathlib import Path\n",
    "\n",
    "from utility import utils as uu\n",
    "from utility.eval import evaluate_classifier_model\n",
    "from utility.confusion_matrix import calculate_confusion_matrix\n",
    "\n",
    "\n",
    "from utility.trainLoopClassifier import training_loop\n",
    "from utility.plotImageModel import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data augmentations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Data augmentation is a technique used to artificially increase the size of a dataset by transforming existing data points to create new, similar instances. This can help prevent overfitting in machine learning models, as well as improve their ability to generalize to unseen data. Common types of data augmentation include flipping, rotation, scaling, and adding noise to images.\n",
    "We can generate the augmentation list with torchvision.transforms module\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augments = torchvision.transforms.Compose([ \n",
    "    torchvision.transforms.RandomHorizontalFlip(p = .5),\n",
    "    torchvision.transforms.RandomVerticalFlip(p = .5),\n",
    "    torchvision.transforms.ColorJitter(brightness=(0.5,1.5), contrast=(1), hue=(-0.1,0.1)),\n",
    "    #torchvision.transforms.RandomCrop((224, 224)), \n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Load the dataset from utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_path = Path(\"plots_and_graphs.ipynb\")\n",
    "parent_dir = cur_path.parent.absolute()\n",
    "masterThesis_folder = str(parent_dir.parent.absolute())+'/'\n",
    "data_dir = masterThesis_folder+\"data/Clean_LiTS/\"\n",
    "\n",
    "cache_me = False\n",
    "if cache_me is True:\n",
    "    cache_mgr = Manager()\n",
    "    cache_mgr.data = cache_mgr.dict()\n",
    "    cache_mgr.cached = cache_mgr.dict()\n",
    "    for k in [\"train\", \"val\", \"test\"]:\n",
    "        cache_mgr.data[k] = cache_mgr.dict()\n",
    "        cache_mgr.cached[k] = False\n",
    "# function from utils, credit: Institute for Artificial Intelligence in Medicine. url: https://mml.ikim.nrw/\n",
    "# dataset outputs a tensor image (dimensions [1,256,256]) and a tensor target (0, 1 or 2)\n",
    "\n",
    "ds = uu.LiTS_Classification_Dataset(\n",
    "    data_dir=data_dir,\n",
    "    transforms=data_augments,\n",
    "    verbose=True,\n",
    "    cache_data=cache_me,\n",
    "    cache_mgr=(cache_mgr if cache_me is True else None),\n",
    "    debug=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default settings\n",
    "batch_size = 32\n",
    "epochs = 50\n",
    "device = (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "time_me  = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataloader\n",
    "dl = torch.utils.data.DataLoader(\n",
    "    dataset = ds, \n",
    "    batch_size = batch_size, \n",
    "    num_workers = 4, \n",
    "    shuffle = True, \n",
    "    drop_last = False, \n",
    "    pin_memory = True,\n",
    "    persistent_workers = (not cache_me),\n",
    "    prefetch_factor = 1\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "ResNet (Residual Network) is a deep neural network architecture introduced in 2015 in [2]. It was designed to address the issue of vanishing gradients in very deep networks. ResNet is named as such because it utilizes residual connections (skip connections), which enable the flow of gradients from earlier layers to later layers, even in very deep networks.\n",
    "\n",
    "The residual connections in ResNet involve adding the input of a layer to the output of a layer that is several layers deeper. This allows the network to more easily learn identity functions. This design helps prevent the issue of vanishing gradients and enables ResNet to train much deeper networks than was previously possible. This architecture has shown significant improvements in benchmarks compared to the earlier AlexNet model.\n",
    "\n",
    "The original ResNet was used in the ImageNet Challenge to classify 1000 classes. However, in our exercise, we only use 3 classes:\n",
    "0: Image does not include the liver.\n",
    "1: Liver is visible.\n",
    "2: Liver is visible and a lesion is visible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# ResNet 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "ResNet50 introduced a new structure called the bottleneck block, which consists of a sequence of [convolution -> batch normalization -> activation] repeated three times. In addition to the 3x3 convolution, the number of channels is also modified within each block using a 1x1 convolution.\n",
    "\n",
    "To provide more flexibility for customization, the variables \"number of channels in,\" \"between,\" and \"out\" are specified for the block. In most cases, the number of input channels will be the same as the number of output channels.\n",
    "\n",
    "A generalized solution is used to address the downsampling issue. Instead of using two different blocks as in ResNet34, a boolean variable and a stride are defined for this purpose. If downsampling is required at the beginning of resblocks3-5, we set \"downsample\" to True and \"stride\" to 2. It's important to note that there is no need to change the stride in the second part of resblocks2.\n",
    "If downsampling is not needed, the operation will be set to nn.Identity()."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "A few words about the torch.nn.init part:\n",
    "PyTorch initializes the parameters for Conv and batch norm randomly from uniform distribution. Initialization of the weights and biases with a normal distribution helps the model backpropagate gradients in early epochs.\n",
    "\n",
    "Tests were conducted on smaller models with 18, 34, and 50 layers, indicating that for adaptive optimizers, weight and bias initialization has minimal effect on model performance or convergence.\n",
    "\n",
    "In contrast, the uniform initialized ResNet 152 model exhibited poor convergence after 15 epochs, with very high error and low accuracy rates. Although initialization improved the performance, it still required tuning of hyperparameters and a better optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ResBlockBottleneck Class\n",
    "#       - constructs a block [conv -> batch_norm -> activation]*3, which we will stack in the network\n",
    "# Input:    int: num_chans_in, int:num_chans_between, int:num_chans_out\n",
    "#           boolean: downsample = False, set True if first block\n",
    "#           int: stride = 1, set 2 if want to downsample\n",
    "# Output:   nn.Sequential() block\n",
    "class ResBlockBottleneck(nn.Module):\n",
    "    def __init__(self, num_chans_in,n_chans_between,num_chans_out, downsample = False, stride = 1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(num_chans_in, n_chans_between, kernel_size=1, padding=0, bias=False)\n",
    "        self.batch_norm1 = nn.BatchNorm2d(num_features=n_chans_between)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.conv2 = nn.Conv2d(n_chans_between, n_chans_between, kernel_size=3, stride= stride, padding=1, bias=False)\n",
    "        self.batch_norm2 = nn.BatchNorm2d(num_features=n_chans_between)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.conv3 = nn.Conv2d(n_chans_between, num_chans_out, kernel_size=1, padding=0, bias=False)\n",
    "        self.batch_norm3 = nn.BatchNorm2d(num_features=num_chans_out)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "\n",
    "        torch.nn.init.kaiming_normal_(self.conv1.weight,\n",
    "                                      nonlinearity='relu')\n",
    "        torch.nn.init.kaiming_normal_(self.conv2.weight,\n",
    "                                      nonlinearity='relu')\n",
    "        torch.nn.init.kaiming_normal_(self.conv3.weight,\n",
    "                                      nonlinearity='relu')\n",
    "\n",
    "        torch.nn.init.constant_(self.batch_norm1.weight, 0.5)\n",
    "        torch.nn.init.zeros_(self.batch_norm1.bias)\n",
    "\n",
    "        torch.nn.init.constant_(self.batch_norm2.weight, 0.5)\n",
    "        torch.nn.init.zeros_(self.batch_norm2.bias)\n",
    "\n",
    "        torch.nn.init.constant_(self.batch_norm3.weight, 0.5)\n",
    "        torch.nn.init.zeros_(self.batch_norm3.bias)\n",
    "\n",
    "        if downsample:\n",
    "            self.downsample = nn.Sequential(\n",
    "                nn.Conv2d(num_chans_in, num_chans_out, kernel_size=1,padding=0,stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(num_features=num_chans_out),\n",
    "                nn.ReLU()\n",
    "            )\n",
    "        else:\n",
    "            self.downsample = nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.batch_norm1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.batch_norm2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv3(out)\n",
    "        out = self.batch_norm3(out)\n",
    "        out = self.relu(out)\n",
    "        return out + self.downsample(x)\n",
    "\n",
    "# ResNetMLMed50 Class\n",
    "#       - constructs a ResNet50 as described in [2, Table 1].\n",
    "# Input:    Tensor: [Batch,1,Height,Width]\n",
    "# Output:   Tensor: [Batch,3]\n",
    "class ResNetMLMed50(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = torch.nn.Conv2d(in_channels = 1, out_channels = 64, kernel_size =7, stride =2, padding=1)\n",
    "        self.batch_norm1 = nn.BatchNorm2d(num_features=64)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.pool2 = torch.nn.MaxPool2d(kernel_size = 3, stride = 2,padding=1)\n",
    "        self.resblocks2 = nn.Sequential(\n",
    "            ResBlockBottleneck(num_chans_in = 64,n_chans_between =64 ,num_chans_out=256,downsample= True),\n",
    "            *(2 * [ResBlockBottleneck(num_chans_in = 256,n_chans_between=64,num_chans_out= 256)]))\n",
    "        self.resblocks3 = nn.Sequential(\n",
    "            ResBlockBottleneck(num_chans_in = 256, n_chans_between=128, num_chans_out= 512, downsample=True, stride=2),\n",
    "            *(3 * [ResBlockBottleneck(num_chans_in = 512,n_chans_between=128,num_chans_out= 512)]))\n",
    "        self.resblocks4 = nn.Sequential(\n",
    "            ResBlockBottleneck(num_chans_in = 512, n_chans_between=256, num_chans_out= 1024,downsample=True, stride=2),\n",
    "            *(5 * [ResBlockBottleneck(num_chans_in = 1024,n_chans_between=256,num_chans_out= 1024)]))\n",
    "        self.resblocks5 = nn.Sequential(\n",
    "            ResBlockBottleneck(num_chans_in = 1024, n_chans_between=512, num_chans_out= 2048,downsample=True, stride=2),\n",
    "            *(2 * [ResBlockBottleneck(num_chans_in = 2048,n_chans_between=512,num_chans_out= 2048)]))\n",
    "        self.avgpool6 = nn.AdaptiveAvgPool2d(output_size=(1, 1))\n",
    "        self.fc = nn.Linear(in_features=2048, out_features=3, bias=True)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        out_1 = self.conv1(x)\n",
    "        out_1 = self.batch_norm1(out_1)\n",
    "        out_1 = self.relu(out_1)\n",
    "        out_1 = self.pool2(out_1)\n",
    "\n",
    "        out_2 = self.resblocks2(out_1)\n",
    "\n",
    "        out_3 = self.resblocks3(out_2)\n",
    "\n",
    "        out_4= self.resblocks4(out_3)\n",
    "\n",
    "        out_5= self.resblocks5(out_4)\n",
    "\n",
    "        out_6 = self.avgpool6(out_5)\n",
    "\n",
    "        out_6= self.fc(torch.flatten(out_6, start_dim=1))\n",
    "        return out_6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = ResNetMLMed50()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "learning_rate = 1e-5\n",
    "run_name = \"ResNet50_fixed_time_lre5\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "uu.csv_logger(\n",
    "        logfile = f\"../logs/{run_name}_hyperparams.csv\",\n",
    "        content = {\"learning_rate\": learning_rate, \"batch_size\": batch_size, \"epochs\": epochs},\n",
    "        first= True,\n",
    "        overwrite= True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "mod_step = 5000\n",
    "wantToPrint = False\n",
    "stop_bool = False\n",
    "eval_test_10_min = False\n",
    "eval_test_15_min = False\n",
    "eval_test_20_min = False\n",
    "skip_test_10_min = False\n",
    "skip_test_15_min = False\n",
    "skip_test_20_min = False"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Modified training loop: The starting time is saved, and the time elapsed is calculated at the beginning of each epoch. If the elapsed time is greater than 10, 15, or 20 minutes, the boolean flag for test evaluation is set to True. However, it is important to ensure that the test evaluation happens only once. Therefore, after the calculation, the boolean flag for skipping is set to True.\n",
    "\n",
    "During the test evaluation, the dataset mode is switched to \"test\", the model is switched to evaluation mode, and the test accuracy, loss, confusion matrix, and per-class accuracy are calculated and saved.\n",
    "\n",
    "The same procedure is repeated three times for each time step."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_start = time.time()\n",
    "\n",
    "num_steps = len(ds.file_names['train'])//batch_size\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    time_elapsed = time.time() - train_start\n",
    "    print(f\"Time_elapsed: {time_elapsed/60 :.2f} min\")\n",
    "    if time_elapsed > 10*60:\n",
    "        eval_test_10_min = True\n",
    "    if time_elapsed > 15*60:\n",
    "        eval_test_15_min = True\n",
    "    if time_elapsed > 20*60:\n",
    "        eval_test_20_min = True\n",
    "\n",
    "    if stop_bool:\n",
    "        print('Stop time')\n",
    "        break\n",
    "\n",
    "    if eval_test_10_min and not skip_test_10_min:\n",
    "        print('Evaluate after first 10 min')\n",
    "        with torch.no_grad():\n",
    "            ds.set_mode(\"test\")\n",
    "            model.eval()\n",
    "            test_accuracy, avg_test_loss = evaluate_classifier_model(model = model, dataloader = dl, device = device)\n",
    "            confusion_matrix, acc = calculate_confusion_matrix(model=model, dataloader=dl, device=device)\n",
    "            torch.save(confusion_matrix, f = 'confusion_matr_' + run_name+ '_10min' + '.pt')\n",
    "            print(f\"Evaluate after first 20 min: Test Loss: {avg_test_loss:.4f}\\t Test Accuracy: {test_accuracy:.4f}, Confusion Matrix: \\n{confusion_matrix}, Per-class Accuracy: {acc}\")\n",
    "            uu.csv_logger(\n",
    "                logfile = f\"../logs/{run_name}_test.csv\",\n",
    "                content = {\"epoch\": epoch,\"test_phase\": 1, \"test_loss\": avg_test_loss, \"test_accuracy\": test_accuracy, \"time_elapsed\": time_elapsed})\n",
    "        skip_test_10_min = True\n",
    "\n",
    "    if eval_test_15_min and not skip_test_15_min:\n",
    "        print('Evaluate after first 15 min')\n",
    "        with torch.no_grad():\n",
    "            ds.set_mode(\"test\")\n",
    "            model.eval()\n",
    "            test_accuracy, avg_test_loss = evaluate_classifier_model(model = model, dataloader = dl, device = device)\n",
    "            confusion_matrix, acc = calculate_confusion_matrix(model=model, dataloader=dl, device=device)\n",
    "            torch.save(confusion_matrix, f = 'confusion_matr_' + run_name+ '_15min' + '.pt')\n",
    "            print(f\"Evaluate after first 20 min: Test Loss: {avg_test_loss:.4f}\\t Test Accuracy: {test_accuracy:.4f}, Confusion Matrix: \\n{confusion_matrix}, Per-class Accuracy: {acc}\")\n",
    "            uu.csv_logger(\n",
    "                logfile = f\"../logs/{run_name}_test.csv\",\n",
    "                content = {\"epoch\": epoch,\"test_phase\": 2, \"test_loss\": avg_test_loss, \"test_accuracy\": test_accuracy, \"time_elapsed\": time_elapsed})\n",
    "        skip_test_15_min = True\n",
    "\n",
    "    if eval_test_20_min and not skip_test_20_min:\n",
    "        print('Evaluate after first 20 min')\n",
    "        with torch.no_grad():\n",
    "            ds.set_mode(\"test\")\n",
    "            model.eval()\n",
    "            test_accuracy, avg_test_loss = evaluate_classifier_model(model = model, dataloader = dl, device = device)\n",
    "            confusion_matrix, acc = calculate_confusion_matrix(model=model, dataloader=dl, device=device)\n",
    "            torch.save(confusion_matrix, f = 'confusion_matr_' + run_name+ '_20min' + '.pt')\n",
    "            print(f\"Evaluate after first 20 min: Test Loss: {avg_test_loss:.4f}\\t Test Accuracy: {test_accuracy:.4f}, Confusion Matrix: \\n{confusion_matrix}, Per-class Accuracy: {acc}\")\n",
    "            uu.csv_logger(\n",
    "                logfile = f\"../logs/{run_name}_test.csv\",\n",
    "                content = {\"epoch\": epoch,\"test_phase\": 3, \"test_loss\": avg_test_loss, \"test_accuracy\": test_accuracy, \"time_elapsed\": time_elapsed})\n",
    "        stop_bool=True\n",
    "        break\n",
    "\n",
    "    # Go to train mode\n",
    "    ds.set_mode(\"train\")\n",
    "    model.train()\n",
    "\n",
    "    # Train loop\n",
    "    for step, (data, targets) in enumerate(dl):\n",
    "\n",
    "        # Manually drop last batch (this is for example relevant with BatchNorm)\n",
    "        if step == num_steps - 1 and (epoch > 0 or ds.cache_data is False):\n",
    "            continue\n",
    "\n",
    "        # Train loop: Zero gradients, forward step, evaluate, log, backward step\n",
    "        optimizer.zero_grad()\n",
    "        data, targets = data.to(device), targets.to(device)\n",
    "        predictions = model(data)\n",
    "        loss = criterion(predictions, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Go to eval mode\n",
    "    ds.set_mode(\"val\")\n",
    "    model.eval()\n",
    "\n",
    "    # Validation loop\n",
    "    val_accuracy, avg_val_loss = evaluate_classifier_model(model = model, dataloader = dl, device = device)\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}]\\t Val Loss: {avg_val_loss:.4f}\\t Val Accuracy: {val_accuracy:.4f}\")\n",
    "    uu.csv_logger(\n",
    "        logfile = f\"../logs/{run_name}_val.csv\",\n",
    "        content = {\"epoch\": epoch, \"val_loss\": avg_val_loss, \"val_accuracy\": val_accuracy},\n",
    "        first = (epoch == 0),\n",
    "        overwrite = (epoch == 0)\n",
    "            )"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "run_name = \"ResNet50_fixed_time_lre4\"\n",
    "learning_rate = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = ResNetMLMed50()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stop_bool = False\n",
    "eval_test_10_min = False\n",
    "eval_test_15_min = False\n",
    "eval_test_20_min = False\n",
    "skip_test_10_min = False\n",
    "skip_test_15_min = False\n",
    "skip_test_20_min = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_start = time.time()\n",
    "\n",
    "num_steps = len(ds.file_names['train'])//batch_size\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    time_elapsed = time.time() - train_start\n",
    "    print(f\"Time_elapsed: {time_elapsed/60 :.2f} min\")\n",
    "    if time_elapsed > 10*60:\n",
    "        eval_test_10_min = True\n",
    "    if time_elapsed > 15*60:\n",
    "        eval_test_15_min = True\n",
    "    if time_elapsed > 20*60:\n",
    "        eval_test_20_min = True\n",
    "\n",
    "    if stop_bool:\n",
    "        print('Stop time')\n",
    "        break\n",
    "\n",
    "    if eval_test_10_min and not skip_test_10_min:\n",
    "        print('Evaluate after first 10 min')\n",
    "        with torch.no_grad():\n",
    "            ds.set_mode(\"test\")\n",
    "            model.eval()\n",
    "            test_accuracy, avg_test_loss = evaluate_classifier_model(model = model, dataloader = dl, device = device)\n",
    "            confusion_matrix, acc = calculate_confusion_matrix(model=model, dataloader=dl, device=device)\n",
    "            torch.save(confusion_matrix, f = 'confusion_matr_' + run_name+ '_10min' + '.pt')\n",
    "            print(f\"Evaluate after first 20 min: Test Loss: {avg_test_loss:.4f}\\t Test Accuracy: {test_accuracy:.4f}, Confusion Matrix: \\n{confusion_matrix}, Per-class Accuracy: {acc}\")\n",
    "            uu.csv_logger(\n",
    "                logfile = f\"../logs/{run_name}_test.csv\",\n",
    "                content = {\"epoch\": epoch,\"test_phase\": 1, \"test_loss\": avg_test_loss, \"test_accuracy\": test_accuracy, \"time_elapsed\": time_elapsed})\n",
    "        skip_test_10_min = True\n",
    "\n",
    "    if eval_test_15_min and not skip_test_15_min:\n",
    "        print('Evaluate after first 15 min')\n",
    "        with torch.no_grad():\n",
    "            ds.set_mode(\"test\")\n",
    "            model.eval()\n",
    "            test_accuracy, avg_test_loss = evaluate_classifier_model(model = model, dataloader = dl, device = device)\n",
    "            confusion_matrix, acc = calculate_confusion_matrix(model=model, dataloader=dl, device=device)\n",
    "            torch.save(confusion_matrix, f = 'confusion_matr_' + run_name+ '_15min' + '.pt')\n",
    "            print(f\"Evaluate after first 20 min: Test Loss: {avg_test_loss:.4f}\\t Test Accuracy: {test_accuracy:.4f}, Confusion Matrix: \\n{confusion_matrix}, Per-class Accuracy: {acc}\")\n",
    "            uu.csv_logger(\n",
    "                logfile = f\"../logs/{run_name}_test.csv\",\n",
    "                content = {\"epoch\": epoch,\"test_phase\": 2, \"test_loss\": avg_test_loss, \"test_accuracy\": test_accuracy, \"time_elapsed\": time_elapsed})\n",
    "        skip_test_15_min = True\n",
    "\n",
    "    if eval_test_20_min and not skip_test_20_min:\n",
    "        print('Evaluate after first 20 min')\n",
    "        with torch.no_grad():\n",
    "            ds.set_mode(\"test\")\n",
    "            model.eval()\n",
    "            test_accuracy, avg_test_loss = evaluate_classifier_model(model = model, dataloader = dl, device = device)\n",
    "            confusion_matrix, acc = calculate_confusion_matrix(model=model, dataloader=dl, device=device)\n",
    "            torch.save(confusion_matrix, f = 'confusion_matr_' + run_name+ '_20min' + '.pt')\n",
    "            print(f\"Evaluate after first 20 min: Test Loss: {avg_test_loss:.4f}\\t Test Accuracy: {test_accuracy:.4f}, Confusion Matrix: \\n{confusion_matrix}, Per-class Accuracy: {acc}\")\n",
    "            uu.csv_logger(\n",
    "                logfile = f\"../logs/{run_name}_test.csv\",\n",
    "                content = {\"epoch\": epoch,\"test_phase\": 3, \"test_loss\": avg_test_loss, \"test_accuracy\": test_accuracy, \"time_elapsed\": time_elapsed})\n",
    "        stop_bool=True\n",
    "        break\n",
    "\n",
    "    # Go to train mode\n",
    "    ds.set_mode(\"train\")\n",
    "    model.train()\n",
    "\n",
    "    # Train loop\n",
    "    for step, (data, targets) in enumerate(dl):\n",
    "\n",
    "        # Manually drop last batch (this is for example relevant with BatchNorm)\n",
    "        if step == num_steps - 1 and (epoch > 0 or ds.cache_data is False):\n",
    "            continue\n",
    "\n",
    "        # Train loop: Zero gradients, forward step, evaluate, log, backward step\n",
    "        optimizer.zero_grad()\n",
    "        data, targets = data.to(device), targets.to(device)\n",
    "        predictions = model(data)\n",
    "        loss = criterion(predictions, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Go to eval mode\n",
    "    ds.set_mode(\"val\")\n",
    "    model.eval()\n",
    "\n",
    "    # Validation loop\n",
    "    val_accuracy, avg_val_loss = evaluate_classifier_model(model = model, dataloader = dl, device = device)\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}]\\t Val Loss: {avg_val_loss:.4f}\\t Val Accuracy: {val_accuracy:.4f}\")\n",
    "    uu.csv_logger(\n",
    "        logfile = f\"../logs/{run_name}_val.csv\",\n",
    "        content = {\"epoch\": epoch, \"val_loss\": avg_val_loss, \"val_accuracy\": val_accuracy},\n",
    "        first = (epoch == 0),\n",
    "        overwrite = (epoch == 0)\n",
    "            )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_name = \"ResNet50_fixed_lre3\"\n",
    "learning_rate = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResNetMLMed50()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_bool = False\n",
    "eval_test_10_min = False\n",
    "eval_test_15_min = False\n",
    "eval_test_20_min = False\n",
    "skip_test_10_min = False\n",
    "skip_test_15_min = False\n",
    "skip_test_20_min = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_start = time.time()\n",
    "\n",
    "num_steps = len(ds.file_names['train'])//batch_size\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    time_elapsed = time.time() - train_start\n",
    "    print(f\"Time_elapsed: {time_elapsed/60 :.2f} min\")\n",
    "    if time_elapsed > 10*60:\n",
    "        eval_test_10_min = True\n",
    "    if time_elapsed > 15*60:\n",
    "        eval_test_15_min = True\n",
    "    if time_elapsed > 20*60:\n",
    "        eval_test_20_min = True\n",
    "\n",
    "    if stop_bool:\n",
    "        print('Stop time')\n",
    "        break\n",
    "\n",
    "    if eval_test_10_min and not skip_test_10_min:\n",
    "        print('Evaluate after first 10 min')\n",
    "        with torch.no_grad():\n",
    "            ds.set_mode(\"test\")\n",
    "            model.eval()\n",
    "            test_accuracy, avg_test_loss = evaluate_classifier_model(model = model, dataloader = dl, device = device)\n",
    "            confusion_matrix, acc = calculate_confusion_matrix(model=model, dataloader=dl, device=device)\n",
    "            torch.save(confusion_matrix, f = 'confusion_matr_' + run_name+ '_10min' + '.pt')\n",
    "            print(f\"Evaluate after first 20 min: Test Loss: {avg_test_loss:.4f}\\t Test Accuracy: {test_accuracy:.4f}, Confusion Matrix: \\n{confusion_matrix}, Per-class Accuracy: {acc}\")\n",
    "            uu.csv_logger(\n",
    "                logfile = f\"../logs/{run_name}_test.csv\",\n",
    "                content = {\"epoch\": epoch,\"test_phase\": 1, \"test_loss\": avg_test_loss, \"test_accuracy\": test_accuracy, \"time_elapsed\": time_elapsed})\n",
    "        skip_test_10_min = True\n",
    "\n",
    "    if eval_test_15_min and not skip_test_15_min:\n",
    "        print('Evaluate after first 15 min')\n",
    "        with torch.no_grad():\n",
    "            ds.set_mode(\"test\")\n",
    "            model.eval()\n",
    "            test_accuracy, avg_test_loss = evaluate_classifier_model(model = model, dataloader = dl, device = device)\n",
    "            confusion_matrix, acc = calculate_confusion_matrix(model=model, dataloader=dl, device=device)\n",
    "            torch.save(confusion_matrix, f = 'confusion_matr_' + run_name+ '_15min' + '.pt')\n",
    "            print(f\"Evaluate after first 20 min: Test Loss: {avg_test_loss:.4f}\\t Test Accuracy: {test_accuracy:.4f}, Confusion Matrix: \\n{confusion_matrix}, Per-class Accuracy: {acc}\")\n",
    "            uu.csv_logger(\n",
    "                logfile = f\"../logs/{run_name}_test.csv\",\n",
    "                content = {\"epoch\": epoch,\"test_phase\": 2, \"test_loss\": avg_test_loss, \"test_accuracy\": test_accuracy, \"time_elapsed\": time_elapsed})\n",
    "        skip_test_15_min = True\n",
    "\n",
    "    if eval_test_20_min and not skip_test_20_min:\n",
    "        print('Evaluate after first 20 min')\n",
    "        with torch.no_grad():\n",
    "            ds.set_mode(\"test\")\n",
    "            model.eval()\n",
    "            test_accuracy, avg_test_loss = evaluate_classifier_model(model = model, dataloader = dl, device = device)\n",
    "            confusion_matrix, acc = calculate_confusion_matrix(model=model, dataloader=dl, device=device)\n",
    "            torch.save(confusion_matrix, f = 'confusion_matr_' + run_name+ '_20min' + '.pt')\n",
    "            print(f\"Evaluate after first 20 min: Test Loss: {avg_test_loss:.4f}\\t Test Accuracy: {test_accuracy:.4f}, Confusion Matrix: \\n{confusion_matrix}, Per-class Accuracy: {acc}\")\n",
    "            uu.csv_logger(\n",
    "                logfile = f\"../logs/{run_name}_test.csv\",\n",
    "                content = {\"epoch\": epoch,\"test_phase\": 3, \"test_loss\": avg_test_loss, \"test_accuracy\": test_accuracy, \"time_elapsed\": time_elapsed})\n",
    "        stop_bool=True\n",
    "        break\n",
    "\n",
    "    # Go to train mode\n",
    "    ds.set_mode(\"train\")\n",
    "    model.train()\n",
    "\n",
    "    # Train loop\n",
    "    for step, (data, targets) in enumerate(dl):\n",
    "\n",
    "        # Manually drop last batch (this is for example relevant with BatchNorm)\n",
    "        if step == num_steps - 1 and (epoch > 0 or ds.cache_data is False):\n",
    "            continue\n",
    "\n",
    "        # Train loop: Zero gradients, forward step, evaluate, log, backward step\n",
    "        optimizer.zero_grad()\n",
    "        data, targets = data.to(device), targets.to(device)\n",
    "        predictions = model(data)\n",
    "        loss = criterion(predictions, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Go to eval mode\n",
    "    ds.set_mode(\"val\")\n",
    "    model.eval()\n",
    "\n",
    "    # Validation loop\n",
    "    val_accuracy, avg_val_loss = evaluate_classifier_model(model = model, dataloader = dl, device = device)\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}]\\t Val Loss: {avg_val_loss:.4f}\\t Val Accuracy: {val_accuracy:.4f}\")\n",
    "    uu.csv_logger(\n",
    "        logfile = f\"../logs/{run_name}_val.csv\",\n",
    "        content = {\"epoch\": epoch, \"val_loss\": avg_val_loss, \"val_accuracy\": val_accuracy},\n",
    "        first = (epoch == 0),\n",
    "        overwrite = (epoch == 0)\n",
    "            )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlcourse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5555b6f2b0077a97828fbbfe12cb97727895c9c472121c1b71224aa97370345d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
