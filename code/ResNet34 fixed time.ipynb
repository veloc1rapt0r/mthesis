{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "This notebook was motivated by\n",
    "\n",
    "[2] Kaiming He et al. ‘Deep Residual Learning for Image Recognition’. In: CoRR abs/1512.03385 (2015). arXiv: 1512.03385.\n",
    "url: http: //arxiv.org/abs/1512.03385.\n",
    "\n",
    "Implementation: Oleh Bakumenko, University of Duisburg-Essen"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "#sys.path.append(\"/datashare/MLCourse/Course_Materials\") # Preferentially import from the datashare.\n",
    "sys.path.append(\"../\") # Otherwise, import from the local folder's parent folder, where your stuff lives.\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import torch, torch.nn as nn\n",
    "import torchvision, torchvision.transforms as tt\n",
    "from torch.multiprocessing import Manager\n",
    "torch.multiprocessing.set_sharing_strategy(\"file_system\")\n",
    "from pathlib import Path\n",
    "\n",
    "from utility import utils as uu\n",
    "from utility.eval import evaluate_classifier_model\n",
    "from utility.confusion_matrix import calculate_confusion_matrix\n",
    "\n",
    "\n",
    "from utility.trainLoopClassifier import training_loop\n",
    "from utility.plotImageModel import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data augmentations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Data augmentation is a technique used to artificially increase the size of a dataset by transforming existing data points to create new, similar instances. This can help prevent overfitting in machine learning models, as well as improve their ability to generalize to unseen data. Common types of data augmentation include flipping, rotation, scaling, and adding noise to images.\n",
    "We can generate the augmentation list with torchvision.transforms module\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augments = torchvision.transforms.Compose([ \n",
    "    torchvision.transforms.RandomHorizontalFlip(p = .5),\n",
    "    torchvision.transforms.RandomVerticalFlip(p = .5),\n",
    "    torchvision.transforms.ColorJitter(brightness=(0.5,1.5), contrast=(1), hue=(-0.1,0.1)),\n",
    "    #torchvision.transforms.RandomCrop((224, 224)), \n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Load the dataset from utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_path = Path(\"plots_and_graphs.ipynb\")\n",
    "parent_dir = cur_path.parent.absolute()\n",
    "masterThesis_folder = str(parent_dir.parent.absolute())+'/'\n",
    "data_dir = masterThesis_folder+\"data/Clean_LiTS/\"\n",
    "\n",
    "cache_me = False\n",
    "if cache_me is True:\n",
    "    cache_mgr = Manager()\n",
    "    cache_mgr.data = cache_mgr.dict()\n",
    "    cache_mgr.cached = cache_mgr.dict()\n",
    "    for k in [\"train\", \"val\", \"test\"]:\n",
    "        cache_mgr.data[k] = cache_mgr.dict()\n",
    "        cache_mgr.cached[k] = False\n",
    "# function from utils, credit: Institute for Artificial Intelligence in Medicine. url: https://mml.ikim.nrw/\n",
    "# dataset outputs a tensor image (dimensions [1,256,256]) and a tensor target (0, 1 or 2)\n",
    "\n",
    "ds = uu.LiTS_Classification_Dataset(\n",
    "    data_dir=data_dir,\n",
    "    transforms=data_augments,\n",
    "    verbose=True,\n",
    "    cache_data=cache_me,\n",
    "    cache_mgr=(cache_mgr if cache_me is True else None),\n",
    "    debug=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default settings\n",
    "batch_size = 32\n",
    "epochs = 50\n",
    "device = (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "time_me  = True"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "The `torch.utils.data.DataLoader` is a utility class in PyTorch that makes the loading and batching of data for training purposes faster. It simplifies the process by allowing us to specify the dataset, batch size (often 32), and whether the data should be shuffled before each epoch. Additionally, there are other parameters available to further customize the data loading process."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataloader\n",
    "dl = torch.utils.data.DataLoader(\n",
    "    dataset = ds, \n",
    "    batch_size = batch_size, \n",
    "    num_workers = 4, \n",
    "    shuffle = True, \n",
    "    drop_last = False, \n",
    "    pin_memory = True,\n",
    "    persistent_workers = (not cache_me),\n",
    "    prefetch_factor = 1\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ResNet 34"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is strongly recommended to parallel look into Table 1 (page 5) and Figure 5 (page 6), ResNetPaper,\n",
    "\n",
    "Implementing the normal ResNet Block = [conv -> batch_norm -> activation] *2\n",
    "\n",
    "At the beginnig of each new layer (in the Table 1, left) the image size will be reduced using convolution with kernel 1 and a stride of 2 (so-called projection), this feature was generalised in the implemention of ResNet 50 below. As an example it was decided to include both variations.\n",
    "\n",
    "First we start with building the blocks. Note the downsampling operation in the ResBlockDimsReduction, because the input image $x$ has different dimentions that the output. If this is not clear, try print(out.shape).\n",
    "\n",
    "Class ResNetMLMed34 will inherit the torch.nn.module, so we need to write the init() and forward() functions. Using the Table 1 and Figure 5 form ResNetPaper we define each resblocks2-5 part, the indexing is the same as in Table 1 so the one can compare number blocks, kernel sizes and number channels.\n",
    "Do not forget to put downsampling block as the first in each resblocks2-5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Couple words about torch.nn.init. part:\n",
    "Pytorch initialise the parameters for Conv and batch norm randomly. Initialization of the weights and biases in a normal distribution helps the model backtrack gradients in early epoch's.\n",
    "For smaller models like 34 and 50 layer it was tested, that initialization of the weights and biases has almost no impact on performance or convergence of the model.\n",
    "\n",
    "For ResNet 152 on the other hand, random initialised model did not converge after 15 epochs and showed very bad error and accuracy rates. With initialization, it still was not great, but may could be tuned by the hyperparameters and better optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ResBlock Class\n",
    "#       - constructs a block [conv -> batch_norm -> activation] *2, which we will stack in the network\n",
    "# Input:    int: n_chans - number channels\n",
    "# Output:   nn.Sequential() block\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, n_chans):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(n_chans, n_chans, kernel_size=3, padding=1, bias= False)\n",
    "        self.batch_norm1 = nn.BatchNorm2d(num_features=n_chans)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.conv2 = nn.Conv2d(n_chans, n_chans, kernel_size=3, padding=1, bias= False)\n",
    "        self.batch_norm2 = nn.BatchNorm2d(num_features=n_chans)\n",
    "\n",
    "        torch.nn.init.kaiming_normal_(self.conv1.weight,\n",
    "                                      nonlinearity='relu')\n",
    "        torch.nn.init.kaiming_normal_(self.conv2.weight,\n",
    "                                      nonlinearity='relu')\n",
    "\n",
    "        torch.nn.init.constant_(self.batch_norm1.weight, 0.5)\n",
    "        torch.nn.init.zeros_(self.batch_norm1.bias)\n",
    "\n",
    "        torch.nn.init.constant_(self.batch_norm2.weight, 0.5)\n",
    "        torch.nn.init.zeros_(self.batch_norm2.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.batch_norm1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.batch_norm2(out)\n",
    "        out = self.relu(out)\n",
    "        return out + x # this sum realise the skip connection\n",
    "\n",
    "\n",
    "# ResBlockDimsReduction Class\n",
    "#       - constructs a first block in the layer\n",
    "#       - [conv -> batch_norm -> activation] *2\n",
    "#       - downsampling performed with stride 2\n",
    "# Input:    int: num_chans_in; int:num_chans_out\n",
    "# Output:   nn.Sequential() block\n",
    "\n",
    "class ResBlockDimsReduction(nn.Module):\n",
    "    def __init__(self, num_chans_in, num_chans_out):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(num_chans_in, num_chans_out, kernel_size=3, stride=2,padding=1,bias= False)\n",
    "        self.batch_norm1 = nn.BatchNorm2d(num_features=num_chans_out)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.conv2 = nn.Conv2d(num_chans_out, num_chans_out, kernel_size=3, padding=1, bias= False)\n",
    "        self.batch_norm2 = nn.BatchNorm2d(num_features=num_chans_out)\n",
    "\n",
    "        torch.nn.init.kaiming_normal_(self.conv1.weight,\n",
    "                                      nonlinearity='relu')\n",
    "        torch.nn.init.kaiming_normal_(self.conv2.weight,\n",
    "                                      nonlinearity='relu')\n",
    "        torch.nn.init.constant_(self.batch_norm1.weight, 0.5)\n",
    "        torch.nn.init.zeros_(self.batch_norm1.bias)\n",
    "        torch.nn.init.constant_(self.batch_norm2.weight, 0.5)\n",
    "        torch.nn.init.zeros_(self.batch_norm2.bias)\n",
    "\n",
    "        self.downsample = nn.Sequential(\n",
    "            nn.Conv2d(num_chans_in, num_chans_out, kernel_size=1, stride=2,bias= False),\n",
    "            nn.BatchNorm2d(num_features=num_chans_out),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.batch_norm1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.batch_norm2(out)\n",
    "        out = self.relu(out)\n",
    "        # input and output dimensions not match, so we need to project x into the dimensions of out\n",
    "        x = self.downsample(x)\n",
    "        return out + x\n",
    "\n",
    "# ResNetMLMed34 Class\n",
    "#       - constructs a ResNet34 as described [2, Table 1].\n",
    "# Input:    Tensor: [Batch,1,Height,Width]\n",
    "# Output:   Tensor: [Batch,3]\n",
    "class ResNetMLMed34(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = torch.nn.Conv2d(in_channels = 1, out_channels = 64, kernel_size =7, stride =2, padding=1, bias= False)\n",
    "        self.batch_norm1 = nn.BatchNorm2d(num_features=64)\n",
    "        self.pool2 = torch.nn.MaxPool2d(kernel_size = 3, stride = 2)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "\n",
    "        self.resblocks2 =nn.Sequential(\n",
    "            *(3 * [ResBlock(n_chans=64)]))\n",
    "        self.resblocks3 = nn.Sequential(ResBlockDimsReduction(num_chans_in=64,num_chans_out=128),\n",
    "            *(3 * [ResBlock(n_chans=128)]))\n",
    "        self.resblocks4 = nn.Sequential(ResBlockDimsReduction(num_chans_in=128,num_chans_out=256),\n",
    "            *(5 * [ResBlock(n_chans=256)]))\n",
    "        self.resblocks5 = nn.Sequential(ResBlockDimsReduction(num_chans_in=256,num_chans_out=512),\n",
    "            *(2 * [ResBlock(n_chans=512)]))\n",
    "        self.avgpool6 = nn.AdaptiveAvgPool2d(output_size=(1, 1))\n",
    "        self.fc = nn.Linear(in_features=512, out_features=3, bias=True)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        out_1 = self.conv1(x)\n",
    "        out_1 = self.batch_norm1(out_1)\n",
    "        out_1 = self.relu(out_1)\n",
    "\n",
    "        out_1 = self.pool2(out_1)\n",
    "\n",
    "        out_2 = self.resblocks2(out_1)\n",
    "\n",
    "        out_3 = self.resblocks3(out_2)\n",
    "\n",
    "        out_4 = self.resblocks4(out_3)\n",
    "\n",
    "        out_5 = self.resblocks5(out_4)\n",
    "\n",
    "        out_6 = self.avgpool6(out_5)\n",
    "\n",
    "        out_6= self.fc(torch.flatten(out_6, start_dim=1))\n",
    "\n",
    "        return out_6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = ResNetMLMed34()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for step, (data, targets) in enumerate(dl):\n",
    "    data, targets = data.to(device), targets.to(device)\n",
    "    if step ==1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "learning_rate = 1e-5\n",
    "run_name = \"ResNet34_fixed_time_lre5\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "uu.csv_logger(\n",
    "        logfile = f\"../logs/{run_name}_hyperparams.csv\",\n",
    "        content = {\"learning_rate\": learning_rate, \"batch_size\": batch_size, \"epochs\": epochs},\n",
    "        first= True,\n",
    "        overwrite= True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "mod_step = 5000\n",
    "wantToPrint = False\n",
    "stop_bool = False\n",
    "eval_test_10_min = False\n",
    "eval_test_15_min = False\n",
    "eval_test_20_min = False\n",
    "skip_test_10_min = False\n",
    "skip_test_15_min = False\n",
    "skip_test_20_min = False"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Modified training loop: The starting time is saved, and the time elapsed is calculated at the beginning of each epoch. If the elapsed time is greater than 10, 15, or 20 minutes, the boolean flag for test evaluation is set to True. However, it is important to ensure that the test evaluation happens only once. Therefore, after the calculation, the boolean flag for skipping is set to True.\n",
    "\n",
    "During the test evaluation, the dataset mode is switched to \"test\", the model is switched to evaluation mode, and the test accuracy, loss, confusion matrix, and per-class accuracy are calculated and saved.\n",
    "\n",
    "The same procedure is repeated three times for each time step."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_start = time.time()\n",
    "\n",
    "num_steps = len(ds.file_names['train'])//batch_size\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    time_elapsed = time.time() - train_start\n",
    "    print(f\"Time_elapsed: {time_elapsed/60 :.2f} min\")\n",
    "    if time_elapsed > 10*60:\n",
    "        eval_test_10_min = True\n",
    "    if time_elapsed > 15*60:\n",
    "        eval_test_15_min = True\n",
    "    if time_elapsed > 20*60:\n",
    "        eval_test_20_min = True\n",
    "\n",
    "    if stop_bool:\n",
    "        print('Stop time')\n",
    "        break\n",
    "\n",
    "    if eval_test_10_min and not skip_test_10_min:\n",
    "        print('Evaluate after first 10 min')\n",
    "        with torch.no_grad():\n",
    "            ds.set_mode(\"test\")\n",
    "            model.eval()\n",
    "            test_accuracy, avg_test_loss = evaluate_classifier_model(model = model, dataloader = dl, device = device)\n",
    "            confusion_matrix, acc = calculate_confusion_matrix(model=model, dataloader=dl, device=device)\n",
    "            torch.save(confusion_matrix, f = 'confusion_matr_' + run_name+ '_10min' + '.pt')\n",
    "            print(f\"Evaluate after first 20 min: Test Loss: {avg_test_loss:.4f}\\t Test Accuracy: {test_accuracy:.4f}, Confusion Matrix: \\n{confusion_matrix}, Per-class Accuracy: {acc}\")\n",
    "            uu.csv_logger(\n",
    "                logfile = f\"../logs/{run_name}_test.csv\",\n",
    "                content = {\"epoch\": epoch,\"test_phase\": 1, \"test_loss\": avg_test_loss, \"test_accuracy\": test_accuracy, \"time_elapsed\": time_elapsed})\n",
    "        skip_test_10_min = True\n",
    "\n",
    "    if eval_test_15_min and not skip_test_15_min:\n",
    "        print('Evaluate after first 15 min')\n",
    "        with torch.no_grad():\n",
    "            ds.set_mode(\"test\")\n",
    "            model.eval()\n",
    "            test_accuracy, avg_test_loss = evaluate_classifier_model(model = model, dataloader = dl, device = device)\n",
    "            confusion_matrix, acc = calculate_confusion_matrix(model=model, dataloader=dl, device=device)\n",
    "            torch.save(confusion_matrix, f = 'confusion_matr_' + run_name+ '_15min' + '.pt')\n",
    "            print(f\"Evaluate after first 20 min: Test Loss: {avg_test_loss:.4f}\\t Test Accuracy: {test_accuracy:.4f}, Confusion Matrix: \\n{confusion_matrix}, Per-class Accuracy: {acc}\")\n",
    "            uu.csv_logger(\n",
    "                logfile = f\"../logs/{run_name}_test.csv\",\n",
    "                content = {\"epoch\": epoch,\"test_phase\": 2, \"test_loss\": avg_test_loss, \"test_accuracy\": test_accuracy, \"time_elapsed\": time_elapsed})\n",
    "        skip_test_15_min = True\n",
    "\n",
    "    if eval_test_20_min and not skip_test_20_min:\n",
    "        print('Evaluate after first 20 min')\n",
    "        with torch.no_grad():\n",
    "            ds.set_mode(\"test\")\n",
    "            model.eval()\n",
    "            test_accuracy, avg_test_loss = evaluate_classifier_model(model = model, dataloader = dl, device = device)\n",
    "            confusion_matrix, acc = calculate_confusion_matrix(model=model, dataloader=dl, device=device)\n",
    "            torch.save(confusion_matrix, f = 'confusion_matr_' + run_name+ '_20min' + '.pt')\n",
    "            print(f\"Evaluate after first 20 min: Test Loss: {avg_test_loss:.4f}\\t Test Accuracy: {test_accuracy:.4f}, Confusion Matrix: \\n{confusion_matrix}, Per-class Accuracy: {acc}\")\n",
    "            uu.csv_logger(\n",
    "                logfile = f\"../logs/{run_name}_test.csv\",\n",
    "                content = {\"epoch\": epoch,\"test_phase\": 3, \"test_loss\": avg_test_loss, \"test_accuracy\": test_accuracy, \"time_elapsed\": time_elapsed})\n",
    "        stop_bool=True\n",
    "        break\n",
    "\n",
    "    # Go to train mode\n",
    "    ds.set_mode(\"train\")\n",
    "    model.train()\n",
    "\n",
    "    # Train loop\n",
    "    for step, (data, targets) in enumerate(dl):\n",
    "\n",
    "        # Manually drop last batch (this is for example relevant with BatchNorm)\n",
    "        if step == num_steps - 1 and (epoch > 0 or ds.cache_data is False):\n",
    "            continue\n",
    "\n",
    "        # Train loop: Zero gradients, forward step, evaluate, log, backward step\n",
    "        optimizer.zero_grad()\n",
    "        data, targets = data.to(device), targets.to(device)\n",
    "        predictions = model(data)\n",
    "        loss = criterion(predictions, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Go to eval mode\n",
    "    ds.set_mode(\"val\")\n",
    "    model.eval()\n",
    "\n",
    "    # Validation loop\n",
    "    val_accuracy, avg_val_loss = evaluate_classifier_model(model = model, dataloader = dl, device = device)\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}]\\t Val Loss: {avg_val_loss:.4f}\\t Val Accuracy: {val_accuracy:.4f}\")\n",
    "    uu.csv_logger(\n",
    "        logfile = f\"../logs/{run_name}_val.csv\",\n",
    "        content = {\"epoch\": epoch, \"val_loss\": avg_val_loss, \"val_accuracy\": val_accuracy},\n",
    "        first = (epoch == 0),\n",
    "        overwrite = (epoch == 0)\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "run_name = \"ResNet34_fixed_time_lre4\"\n",
    "learning_rate = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = ResNetMLMed34()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stop_bool = False\n",
    "eval_test_10_min = False\n",
    "eval_test_15_min = False\n",
    "eval_test_20_min = False\n",
    "skip_test_10_min = False\n",
    "skip_test_15_min = False\n",
    "skip_test_20_min = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_start = time.time()\n",
    "\n",
    "num_steps = len(ds.file_names['train'])//batch_size\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    time_elapsed = time.time() - train_start\n",
    "    print(f\"Time_elapsed: {time_elapsed/60 :.2f} min\")\n",
    "    if time_elapsed > 10*60:\n",
    "        eval_test_10_min = True\n",
    "    if time_elapsed > 15*60:\n",
    "        eval_test_15_min = True\n",
    "    if time_elapsed > 20*60:\n",
    "        eval_test_20_min = True\n",
    "\n",
    "    if stop_bool:\n",
    "        print('Stop time')\n",
    "        break\n",
    "\n",
    "    if eval_test_10_min and not skip_test_10_min:\n",
    "        print('Evaluate after first 10 min')\n",
    "        with torch.no_grad():\n",
    "            ds.set_mode(\"test\")\n",
    "            model.eval()\n",
    "            test_accuracy, avg_test_loss = evaluate_classifier_model(model = model, dataloader = dl, device = device)\n",
    "            confusion_matrix, acc = calculate_confusion_matrix(model=model, dataloader=dl, device=device)\n",
    "            torch.save(confusion_matrix, f = 'confusion_matr_' + run_name+ '_10min' + '.pt')\n",
    "            print(f\"Evaluate after first 20 min: Test Loss: {avg_test_loss:.4f}\\t Test Accuracy: {test_accuracy:.4f}, Confusion Matrix: \\n{confusion_matrix}, Per-class Accuracy: {acc}\")\n",
    "            uu.csv_logger(\n",
    "                logfile = f\"../logs/{run_name}_test.csv\",\n",
    "                content = {\"epoch\": epoch,\"test_phase\": 1, \"test_loss\": avg_test_loss, \"test_accuracy\": test_accuracy, \"time_elapsed\": time_elapsed})\n",
    "        skip_test_10_min = True\n",
    "\n",
    "    if eval_test_15_min and not skip_test_15_min:\n",
    "        print('Evaluate after first 15 min')\n",
    "        with torch.no_grad():\n",
    "            ds.set_mode(\"test\")\n",
    "            model.eval()\n",
    "            test_accuracy, avg_test_loss = evaluate_classifier_model(model = model, dataloader = dl, device = device)\n",
    "            confusion_matrix, acc = calculate_confusion_matrix(model=model, dataloader=dl, device=device)\n",
    "            torch.save(confusion_matrix, f = 'confusion_matr_' + run_name+ '_15min' + '.pt')\n",
    "            print(f\"Evaluate after first 20 min: Test Loss: {avg_test_loss:.4f}\\t Test Accuracy: {test_accuracy:.4f}, Confusion Matrix: \\n{confusion_matrix}, Per-class Accuracy: {acc}\")\n",
    "            uu.csv_logger(\n",
    "                logfile = f\"../logs/{run_name}_test.csv\",\n",
    "                content = {\"epoch\": epoch,\"test_phase\": 2, \"test_loss\": avg_test_loss, \"test_accuracy\": test_accuracy, \"time_elapsed\": time_elapsed})\n",
    "        skip_test_15_min = True\n",
    "\n",
    "    if eval_test_20_min and not skip_test_20_min:\n",
    "        print('Evaluate after first 20 min')\n",
    "        with torch.no_grad():\n",
    "            ds.set_mode(\"test\")\n",
    "            model.eval()\n",
    "            test_accuracy, avg_test_loss = evaluate_classifier_model(model = model, dataloader = dl, device = device)\n",
    "            confusion_matrix, acc = calculate_confusion_matrix(model=model, dataloader=dl, device=device)\n",
    "            torch.save(confusion_matrix, f = 'confusion_matr_' + run_name+ '_20min' + '.pt')\n",
    "            print(f\"Evaluate after first 20 min: Test Loss: {avg_test_loss:.4f}\\t Test Accuracy: {test_accuracy:.4f}, Confusion Matrix: \\n{confusion_matrix}, Per-class Accuracy: {acc}\")\n",
    "            uu.csv_logger(\n",
    "                logfile = f\"../logs/{run_name}_test.csv\",\n",
    "                content = {\"epoch\": epoch,\"test_phase\": 3, \"test_loss\": avg_test_loss, \"test_accuracy\": test_accuracy, \"time_elapsed\": time_elapsed})\n",
    "        stop_bool=True\n",
    "        break\n",
    "\n",
    "    # Go to train mode\n",
    "    ds.set_mode(\"train\")\n",
    "    model.train()\n",
    "\n",
    "    # Train loop\n",
    "    for step, (data, targets) in enumerate(dl):\n",
    "\n",
    "        # Manually drop last batch (this is for example relevant with BatchNorm)\n",
    "        if step == num_steps - 1 and (epoch > 0 or ds.cache_data is False):\n",
    "            continue\n",
    "\n",
    "        # Train loop: Zero gradients, forward step, evaluate, log, backward step\n",
    "        optimizer.zero_grad()\n",
    "        data, targets = data.to(device), targets.to(device)\n",
    "        predictions = model(data)\n",
    "        loss = criterion(predictions, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Go to eval mode\n",
    "    ds.set_mode(\"val\")\n",
    "    model.eval()\n",
    "\n",
    "    # Validation loop\n",
    "    val_accuracy, avg_val_loss = evaluate_classifier_model(model = model, dataloader = dl, device = device)\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}]\\t Val Loss: {avg_val_loss:.4f}\\t Val Accuracy: {val_accuracy:.4f}\")\n",
    "    uu.csv_logger(\n",
    "        logfile = f\"../logs/{run_name}_val.csv\",\n",
    "        content = {\"epoch\": epoch, \"val_loss\": avg_val_loss, \"val_accuracy\": val_accuracy},\n",
    "        first = (epoch == 0),\n",
    "        overwrite = (epoch == 0)\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "run_name = \"ResNet34_fixed_time_rerun_1004_lre3\"\n",
    "learning_rate = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = ResNetMLMed34()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stop_bool = False\n",
    "eval_test_10_min = False\n",
    "eval_test_15_min = False\n",
    "eval_test_20_min = False\n",
    "skip_test_10_min = False\n",
    "skip_test_15_min = False\n",
    "skip_test_20_min = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_start = time.time()\n",
    "\n",
    "num_steps = len(ds.file_names['train'])//batch_size\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    time_elapsed = time.time() - train_start\n",
    "    print(f\"Time_elapsed: {time_elapsed/60 :.2f} min\")\n",
    "    if time_elapsed > 10*60:\n",
    "        eval_test_10_min = True\n",
    "    if time_elapsed > 15*60:\n",
    "        eval_test_15_min = True\n",
    "    if time_elapsed > 20*60:\n",
    "        eval_test_20_min = True\n",
    "\n",
    "    if stop_bool:\n",
    "        print('Stop time')\n",
    "        break\n",
    "\n",
    "    if eval_test_10_min and not skip_test_10_min:\n",
    "        print('Evaluate after first 10 min')\n",
    "        with torch.no_grad():\n",
    "            ds.set_mode(\"test\")\n",
    "            model.eval()\n",
    "            test_accuracy, avg_test_loss = evaluate_classifier_model(model = model, dataloader = dl, device = device)\n",
    "            confusion_matrix, acc = calculate_confusion_matrix(model=model, dataloader=dl, device=device)\n",
    "            torch.save(confusion_matrix, f = 'confusion_matr_' + run_name+ '_10min' + '.pt')\n",
    "            print(f\"Evaluate after first 20 min: Test Loss: {avg_test_loss:.4f}\\t Test Accuracy: {test_accuracy:.4f}, Confusion Matrix: \\n{confusion_matrix}, Per-class Accuracy: {acc}\")\n",
    "            uu.csv_logger(\n",
    "                logfile = f\"../logs/{run_name}_test.csv\",\n",
    "                content = {\"epoch\": epoch,\"test_phase\": 1, \"test_loss\": avg_test_loss, \"test_accuracy\": test_accuracy, \"time_elapsed\": time_elapsed})\n",
    "        skip_test_10_min = True\n",
    "\n",
    "    if eval_test_15_min and not skip_test_15_min:\n",
    "        print('Evaluate after first 15 min')\n",
    "        with torch.no_grad():\n",
    "            ds.set_mode(\"test\")\n",
    "            model.eval()\n",
    "            test_accuracy, avg_test_loss = evaluate_classifier_model(model = model, dataloader = dl, device = device)\n",
    "            confusion_matrix, acc = calculate_confusion_matrix(model=model, dataloader=dl, device=device)\n",
    "            torch.save(confusion_matrix, f = 'confusion_matr_' + run_name+ '_15min' + '.pt')\n",
    "            print(f\"Evaluate after first 20 min: Test Loss: {avg_test_loss:.4f}\\t Test Accuracy: {test_accuracy:.4f}, Confusion Matrix: \\n{confusion_matrix}, Per-class Accuracy: {acc}\")\n",
    "            uu.csv_logger(\n",
    "                logfile = f\"../logs/{run_name}_test.csv\",\n",
    "                content = {\"epoch\": epoch,\"test_phase\": 2, \"test_loss\": avg_test_loss, \"test_accuracy\": test_accuracy, \"time_elapsed\": time_elapsed})\n",
    "        skip_test_15_min = True\n",
    "\n",
    "    if eval_test_20_min and not skip_test_20_min:\n",
    "        print('Evaluate after first 20 min')\n",
    "        with torch.no_grad():\n",
    "            ds.set_mode(\"test\")\n",
    "            model.eval()\n",
    "            test_accuracy, avg_test_loss = evaluate_classifier_model(model = model, dataloader = dl, device = device)\n",
    "            confusion_matrix, acc = calculate_confusion_matrix(model=model, dataloader=dl, device=device)\n",
    "            torch.save(confusion_matrix, f = 'confusion_matr_' + run_name+ '_20min' + '.pt')\n",
    "            print(f\"Evaluate after first 20 min: Test Loss: {avg_test_loss:.4f}\\t Test Accuracy: {test_accuracy:.4f}, Confusion Matrix: \\n{confusion_matrix}, Per-class Accuracy: {acc}\")\n",
    "            uu.csv_logger(\n",
    "                logfile = f\"../logs/{run_name}_test.csv\",\n",
    "                content = {\"epoch\": epoch,\"test_phase\": 3, \"test_loss\": avg_test_loss, \"test_accuracy\": test_accuracy, \"time_elapsed\": time_elapsed})\n",
    "        stop_bool=True\n",
    "        break\n",
    "\n",
    "    # Go to train mode\n",
    "    ds.set_mode(\"train\")\n",
    "    model.train()\n",
    "\n",
    "    # Train loop\n",
    "    for step, (data, targets) in enumerate(dl):\n",
    "\n",
    "        # Manually drop last batch (this is for example relevant with BatchNorm)\n",
    "        if step == num_steps - 1 and (epoch > 0 or ds.cache_data is False):\n",
    "            continue\n",
    "\n",
    "        # Train loop: Zero gradients, forward step, evaluate, log, backward step\n",
    "        optimizer.zero_grad()\n",
    "        data, targets = data.to(device), targets.to(device)\n",
    "        predictions = model(data)\n",
    "        loss = criterion(predictions, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Go to eval mode\n",
    "    ds.set_mode(\"val\")\n",
    "    model.eval()\n",
    "\n",
    "    # Validation loop\n",
    "    val_accuracy, avg_val_loss = evaluate_classifier_model(model = model, dataloader = dl, device = device)\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}]\\t Val Loss: {avg_val_loss:.4f}\\t Val Accuracy: {val_accuracy:.4f}\")\n",
    "    uu.csv_logger(\n",
    "        logfile = f\"../logs/{run_name}_val.csv\",\n",
    "        content = {\"epoch\": epoch, \"val_loss\": avg_val_loss, \"val_accuracy\": val_accuracy},\n",
    "        first = (epoch == 0),\n",
    "        overwrite = (epoch == 0)\n",
    "            )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlcourse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5555b6f2b0077a97828fbbfe12cb97727895c9c472121c1b71224aa97370345d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
