{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook was motivated by\n",
    "\n",
    "[3] Karen Simonyan and Andrew Zisserman. ‘Very Deep Convolutional Networks for Large-Scale Image Recognition’. In: (2014). doi: 10. 48550/ARXIV.1409.1556. url: https://arxiv.org/abs/1409.1556.\n",
    "\n",
    "Implementation: Oleh Bakumenko, University of Duisburg-Essen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import torch, torch.nn as nn\n",
    "import torchvision, torchvision.transforms as tt\n",
    "from torchsummary import summary\n",
    "from torch.multiprocessing import Manager\n",
    "torch.multiprocessing.set_sharing_strategy(\"file_system\")\n",
    "from pathlib import Path\n",
    "\n",
    "from utility import utils as uu\n",
    "from utility.eval import evaluate_classifier_model\n",
    "from utility.confusion_matrix import calculate_confusion_matrix\n",
    "from utility.trainLoopClassifier import *\n",
    "from utility.plotImageModel import *"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Data augmentation is a technique used to artificially increase the size of a dataset by transforming existing data points to create new, similar instances. This can help prevent overfitting in machine learning models, as well as improve their ability to generalize to unseen data. Common types of data augmentation include flipping, rotation, scaling, and adding noise to images.\n",
    "We can generate the augmentation list with torchvision.transforms module\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "data_augments = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.RandomHorizontalFlip(p = .5),\n",
    "    torchvision.transforms.RandomVerticalFlip(p = .5),\n",
    "    torchvision.transforms.ColorJitter(brightness=(0.5,1.5), contrast=(1), hue=(-0.1,0.1)),\n",
    "    #torchvision.transforms.RandomCrop((224, 224)),\n",
    "    ])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the dataset from utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f35875cfaf684878abc74a342e92299c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/3039 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "605a24ddb88040fe95e4d8f64288fb5e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/3038 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3520f7c8b87d4a6f9d8fbf0c28bec284"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "11e801018e7b46869614389555294ebe"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/3039 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "68315b939911406682c34d60595fef4b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/3038 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "77de1ee5e6934423a2ab031fb0b96125"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset initialization complete.\n"
     ]
    }
   ],
   "source": [
    "cur_path = Path(\"plots_and_graphs.ipynb\")\n",
    "parent_dir = cur_path.parent.absolute()\n",
    "masterThesis_folder = str(parent_dir.parent.absolute())+'/'\n",
    "data_dir = masterThesis_folder+\"data/Clean_LiTS/\"\n",
    "\n",
    "cache_me = False\n",
    "if cache_me is True:\n",
    "    cache_mgr = Manager()\n",
    "    cache_mgr.data = cache_mgr.dict()\n",
    "    cache_mgr.cached = cache_mgr.dict()\n",
    "    for k in [\"train\", \"val\", \"test\"]:\n",
    "        cache_mgr.data[k] = cache_mgr.dict()\n",
    "        cache_mgr.cached[k] = False\n",
    "# function from utils, credit: Institute for Artificial Intelligence in Medicine. url: https://mml.ikim.nrw/\n",
    "# dataset outputs a tensor image (dimensions [1,256,256]) and a tensor target (0, 1 or 2)\n",
    "\n",
    "ds = uu.LiTS_Classification_Dataset(\n",
    "    data_dir=data_dir,\n",
    "    transforms=data_augments,\n",
    "    verbose=True,\n",
    "    cache_data=cache_me,\n",
    "    cache_mgr=(cache_mgr if cache_me is True else None),\n",
    "    debug=True,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Hyperparameters\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default settings\n",
    "batch_size = 32\n",
    "learning_rate = 1e-4\n",
    "weight_decay = 1e-5\n",
    "epochs = 15\n",
    "run_name = \"VGG19\"\n",
    "device = (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "time_me = True"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "The `torch.utils.data.DataLoader` is a utility class in PyTorch that makes the loading and batching of data for training purposes faster. It simplifies the process by allowing us to specify the dataset, batch size (often 32), and whether the data should be shuffled before each epoch. Additionally, there are other parameters available to further customize the data loading process."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataloader\n",
    "dl = torch.utils.data.DataLoader(\n",
    "    dataset = ds, \n",
    "    batch_size = batch_size, \n",
    "    num_workers = 4, \n",
    "    shuffle = True, \n",
    "    drop_last = False, \n",
    "    pin_memory = True,\n",
    "    persistent_workers = (not cache_me),\n",
    "    prefetch_factor = 1\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VGG\n",
    "\n",
    "VGG19 is a convolutional neural network architecture that consists of 19 layers, including 16 convolutional layers and 3 fully connected layers. It was developed by the Visual Geometry Group at the University of Oxford and has been widely used for image classification tasks. The network's architecture is characterized by the use of small convolutional filters (3x3) with a stride of 1, and the use of max pooling layers with a filter size of 2x2 and a stride of 2. VGG19 has achieved high accuracy on benchmark image classification datasets such as ImageNet.\n",
    "\n",
    "The original network was used in the ImageNet Challenge to classify 1000 classes. However, in our exercise, we only use 3 classes:\n",
    "0: Image does not include the liver.\n",
    "1: Liver is visible.\n",
    "2: Liver is visible and a lesion is visible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# VGGBlock Class\n",
    "#       - constructs a block [conv -> relu], which we will stack in the network\n",
    "# Input:    int: n_chans - number channels\n",
    "# Output:   nn.Sequential() block\n",
    "\n",
    "class VGGBlock(nn.Module):\n",
    "    def __init__(self, n_chans):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(n_chans, n_chans, kernel_size=3, padding=1)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        torch.nn.init.kaiming_normal_(self.conv1.weight,nonlinearity='relu')\n",
    "    def forward(self, x):\n",
    "        out = self.relu(self.conv1(x))\n",
    "        return out"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# VGG19MLMed18 Class\n",
    "#       - constructs a VGG19 as described in [3, Table 1]\n",
    "# Input:    Tensor: [Batch,1,Height,Width]\n",
    "# Output:   Tensor: [Batch,3]\n",
    "class VGG19MLMed(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.max_pool = torch.nn.MaxPool2d(kernel_size = 2, stride = 2, padding=0)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "\n",
    "        self.conv1 = torch.nn.Conv2d(in_channels = 1,   out_channels = 64, kernel_size =3, stride =1, padding=1)\n",
    "        self.layer1 = VGGBlock(n_chans=64)\n",
    "        self.conv2 = torch.nn.Conv2d(in_channels = 64,  out_channels = 128, kernel_size =3, stride =1, padding=1)\n",
    "        self.layer2 = VGGBlock(n_chans=128)\n",
    "        self.conv3 = torch.nn.Conv2d(in_channels = 128, out_channels = 256, kernel_size =3, stride =1, padding=1)\n",
    "        self.layer3 = nn.Sequential(*(3 * [VGGBlock(n_chans=256)]))\n",
    "        self.conv4 = torch.nn.Conv2d(in_channels = 256, out_channels = 512, kernel_size =3, stride =1, padding=1)\n",
    "        self.layer4 = nn.Sequential(*(3 * [VGGBlock(n_chans=512)]))\n",
    "\n",
    "        self.layer5 = nn.Sequential(*(4 * [VGGBlock(n_chans=512)]))\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((7, 7))\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(in_features=25088, out_features=4096, bias=True),\n",
    "            torch.nn.ReLU(),\n",
    "            nn.Dropout(p = 0.5),\n",
    "            nn.Linear(in_features=4096, out_features=4096, bias=True),\n",
    "            torch.nn.ReLU(),\n",
    "            nn.Dropout(p = 0.5),\n",
    "            nn.Linear(in_features=4096, out_features=3, bias=True),\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        out_1 = self.relu(self.conv1(x))\n",
    "        out_1 = self.max_pool(self.layer1(out_1))\n",
    "\n",
    "        out_2 = self.relu(self.conv2(out_1))\n",
    "        out_2 = self.max_pool(self.layer2(out_2))\n",
    "\n",
    "        out_3 = self.relu(self.conv3(out_2))\n",
    "        out_3 = self.max_pool(self.layer3(out_3))\n",
    "\n",
    "        out_4 = self.relu(self.conv4(out_3))\n",
    "        out_4 = self.max_pool(self.layer4(out_4))\n",
    "\n",
    "        out_5 = self.max_pool(self.layer5(out_4))\n",
    "        out_6 = self.avgpool(out_5)\n",
    "        out_6 = torch.flatten(out_6, 1)\n",
    "\n",
    "        out_6= self.fc(out_6)\n",
    "\n",
    "        return out_6"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "model = VGG19MLMed()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 256, 256]             640\n",
      "              ReLU-2         [-1, 64, 256, 256]               0\n",
      "            Conv2d-3         [-1, 64, 256, 256]          36,928\n",
      "              ReLU-4         [-1, 64, 256, 256]               0\n",
      "          VGGBlock-5         [-1, 64, 256, 256]               0\n",
      "         MaxPool2d-6         [-1, 64, 128, 128]               0\n",
      "            Conv2d-7        [-1, 128, 128, 128]          73,856\n",
      "              ReLU-8        [-1, 128, 128, 128]               0\n",
      "            Conv2d-9        [-1, 128, 128, 128]         147,584\n",
      "             ReLU-10        [-1, 128, 128, 128]               0\n",
      "         VGGBlock-11        [-1, 128, 128, 128]               0\n",
      "        MaxPool2d-12          [-1, 128, 64, 64]               0\n",
      "           Conv2d-13          [-1, 256, 64, 64]         295,168\n",
      "             ReLU-14          [-1, 256, 64, 64]               0\n",
      "           Conv2d-15          [-1, 256, 64, 64]         590,080\n",
      "             ReLU-16          [-1, 256, 64, 64]               0\n",
      "         VGGBlock-17          [-1, 256, 64, 64]               0\n",
      "           Conv2d-18          [-1, 256, 64, 64]         590,080\n",
      "             ReLU-19          [-1, 256, 64, 64]               0\n",
      "         VGGBlock-20          [-1, 256, 64, 64]               0\n",
      "           Conv2d-21          [-1, 256, 64, 64]         590,080\n",
      "             ReLU-22          [-1, 256, 64, 64]               0\n",
      "         VGGBlock-23          [-1, 256, 64, 64]               0\n",
      "        MaxPool2d-24          [-1, 256, 32, 32]               0\n",
      "           Conv2d-25          [-1, 512, 32, 32]       1,180,160\n",
      "             ReLU-26          [-1, 512, 32, 32]               0\n",
      "           Conv2d-27          [-1, 512, 32, 32]       2,359,808\n",
      "             ReLU-28          [-1, 512, 32, 32]               0\n",
      "         VGGBlock-29          [-1, 512, 32, 32]               0\n",
      "           Conv2d-30          [-1, 512, 32, 32]       2,359,808\n",
      "             ReLU-31          [-1, 512, 32, 32]               0\n",
      "         VGGBlock-32          [-1, 512, 32, 32]               0\n",
      "           Conv2d-33          [-1, 512, 32, 32]       2,359,808\n",
      "             ReLU-34          [-1, 512, 32, 32]               0\n",
      "         VGGBlock-35          [-1, 512, 32, 32]               0\n",
      "        MaxPool2d-36          [-1, 512, 16, 16]               0\n",
      "           Conv2d-37          [-1, 512, 16, 16]       2,359,808\n",
      "             ReLU-38          [-1, 512, 16, 16]               0\n",
      "         VGGBlock-39          [-1, 512, 16, 16]               0\n",
      "           Conv2d-40          [-1, 512, 16, 16]       2,359,808\n",
      "             ReLU-41          [-1, 512, 16, 16]               0\n",
      "         VGGBlock-42          [-1, 512, 16, 16]               0\n",
      "           Conv2d-43          [-1, 512, 16, 16]       2,359,808\n",
      "             ReLU-44          [-1, 512, 16, 16]               0\n",
      "         VGGBlock-45          [-1, 512, 16, 16]               0\n",
      "           Conv2d-46          [-1, 512, 16, 16]       2,359,808\n",
      "             ReLU-47          [-1, 512, 16, 16]               0\n",
      "         VGGBlock-48          [-1, 512, 16, 16]               0\n",
      "        MaxPool2d-49            [-1, 512, 8, 8]               0\n",
      "AdaptiveAvgPool2d-50            [-1, 512, 7, 7]               0\n",
      "           Linear-51                 [-1, 4096]     102,764,544\n",
      "             ReLU-52                 [-1, 4096]               0\n",
      "          Dropout-53                 [-1, 4096]               0\n",
      "           Linear-54                 [-1, 4096]      16,781,312\n",
      "             ReLU-55                 [-1, 4096]               0\n",
      "          Dropout-56                 [-1, 4096]               0\n",
      "           Linear-57                    [-1, 3]          12,291\n",
      "================================================================\n",
      "Total params: 139,581,379\n",
      "Trainable params: 139,581,379\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.25\n",
      "Forward/backward pass size (MB): 399.63\n",
      "Params size (MB): 532.46\n",
      "Estimated Total Size (MB): 932.34\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(VGG19MLMed(), (1, 256, 256))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for step, (data, targets) in enumerate(dl):\n",
    "    data, targets = data.to(device), targets.to(device)\n",
    "    if step ==1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([2, 3])"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = model.to(device)\n",
    "model(data).shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate, weight_decay = weight_decay)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "training_loop(\n",
    "    epochs = epochs,\n",
    "    optimizer = optimizer,\n",
    "    model = model,\n",
    "    criterion = criterion,\n",
    "    ds = ds,\n",
    "    dl = dl,\n",
    "    batch_size = batch_size,\n",
    "    run_name = run_name,\n",
    "    cache_me = cache_me,\n",
    "    device = device,\n",
    "    mod_step = 500,\n",
    "    time_me = True,\n",
    "    time = time)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlcourse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 (main, Aug 25 2022, 23:26:10) \n[GCC 11.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5555b6f2b0077a97828fbbfe12cb97727895c9c472121c1b71224aa97370345d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
