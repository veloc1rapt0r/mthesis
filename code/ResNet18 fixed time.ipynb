{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "This notebook was motivated by\n",
    "\n",
    "[2] Kaiming He et al. ‘Deep Residual Learning for Image Recognition’. In: CoRR abs/1512.03385 (2015). arXiv: 1512.03385.\n",
    "url: http: //arxiv.org/abs/1512.03385.\n",
    "\n",
    "Implementation: Oleh Bakumenko, University of Duisburg-Essen"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import torch, torch.nn as nn\n",
    "import torchvision, torchvision.transforms as tt\n",
    "from torch.multiprocessing import Manager\n",
    "torch.multiprocessing.set_sharing_strategy(\"file_system\")\n",
    "from utility import utils as uu\n",
    "from utility.eval import evaluate_classifier_model\n",
    "from utility.confusion_matrix import calculate_confusion_matrix\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "from utility.trainLoopClassifier import *\n",
    "from utility.plotImageModel import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data augmentations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Data augmentation is a technique used to artificially increase the size of a dataset by transforming existing data points to create new, similar instances. This can help prevent overfitting in machine learning models, as well as improve their ability to generalize to unseen data. Common types of data augmentation include flipping, rotation, scaling, and adding noise to images.\n",
    "We can generate the augmentation list with torchvision.transforms module\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augments = torchvision.transforms.Compose([ \n",
    "    torchvision.transforms.RandomHorizontalFlip(p = .5),\n",
    "    torchvision.transforms.RandomVerticalFlip(p = .5),\n",
    "    torchvision.transforms.ColorJitter(brightness=(0.5,1.5), contrast=(1), hue=(-0.1,0.1)),\n",
    "    #torchvision.transforms.RandomCrop((224, 224)), \n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Load the dataset from utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9d4d1cc2b48b4679ba4e99ddd5ac2d5c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/3039 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7df35936b9b941cab4f82fd368a90650"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/3038 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "475606eae87d45da974902df254c3f43"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ef2efaf366eb4675abc421b55dd1180c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/3039 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "45a943340d344ba79f6dbc4c2dddd637"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/3038 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f1fcd975b9eb47e2a69fe315c56e5175"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset initialization complete.\n"
     ]
    }
   ],
   "source": [
    "cur_path = Path(\"plots_and_graphs.ipynb\")\n",
    "parent_dir = cur_path.parent.absolute()\n",
    "masterThesis_folder = str(parent_dir.parent.absolute())+'/'\n",
    "data_dir = masterThesis_folder+\"data/Clean_LiTS/\"\n",
    "\n",
    "cache_me = False\n",
    "if cache_me is True:\n",
    "    cache_mgr = Manager()\n",
    "    cache_mgr.data = cache_mgr.dict()\n",
    "    cache_mgr.cached = cache_mgr.dict()\n",
    "    for k in [\"train\", \"val\", \"test\"]:\n",
    "        cache_mgr.data[k] = cache_mgr.dict()\n",
    "        cache_mgr.cached[k] = False\n",
    "# function from utils, credit: Institute for Artificial Intelligence in Medicine. url: https://mml.ikim.nrw/\n",
    "# dataset outputs a tensor image (dimensions [1,256,256]) and a tensor target (0, 1 or 2)\n",
    "\n",
    "ds = uu.LiTS_Classification_Dataset(\n",
    "    data_dir=data_dir,\n",
    "    transforms=data_augments,\n",
    "    verbose=True,\n",
    "    cache_data=cache_me,\n",
    "    cache_mgr=(cache_mgr if cache_me is True else None),\n",
    "    debug=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default settings\n",
    "batch_size = 32\n",
    "epochs = 50\n",
    "device = (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "time_me  = True"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "The `torch.utils.data.DataLoader` is a utility class in PyTorch that makes the loading and batching of data for training purposes faster. It simplifies the process by allowing us to specify the dataset, batch size (often 32), and whether the data should be shuffled before each epoch. Additionally, there are other parameters available to further customize the data loading process."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataloader\n",
    "dl = torch.utils.data.DataLoader(\n",
    "    dataset = ds, \n",
    "    batch_size = batch_size, \n",
    "    num_workers = 4, \n",
    "    shuffle = True, \n",
    "    drop_last = False, \n",
    "    pin_memory = True,\n",
    "    persistent_workers = (not cache_me),\n",
    "    prefetch_factor = 1\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "ResNet (Residual Network) is a deep neural network architecture introduced in 2015 in [2]. It was designed to address the issue of vanishing gradients in very deep networks. ResNet is named as such because it utilizes residual connections (skip connections), which enable the flow of gradients from earlier layers to later layers, even in very deep networks.\n",
    "\n",
    "The residual connections in ResNet involve adding the input of a layer to the output of a layer that is several layers deeper. This allows the network to more easily learn identity functions. This design helps prevent the issue of vanishing gradients and enables ResNet to train much deeper networks than was previously possible. This architecture has shown significant improvements in benchmarks compared to the earlier AlexNet model.\n",
    "\n",
    "The original ResNet was used in the ImageNet Challenge to classify 1000 classes. However, in our exercise, we only use 3 classes:\n",
    "0: Image does not include the liver.\n",
    "1: Liver is visible.\n",
    "2: Liver is visible and a lesion is visible."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# ResNet 18"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We highly recommend cross-referencing Table 1 on page 5 and Figure 5 on page 6 of reference [2] simultaneously.\n",
    "\n",
    "To implement the normal ResNet Block, we use the following sequence: [conv -> batch_norm -> activation] * 2.\n",
    "\n",
    "At the beginning of each new layer (as shown in Table 1, left), the image size is reduced using convolution with a kernel size of 1 and a stride of 2 (known as projection). This feature was generalized in the implementation of ResNet 50. As an example, we have decided to include both variations.\n",
    "\n",
    "First, we start by building the blocks. Please note the downsampling operation in the ResBlockDimsReduction, as the input image $x$ has different dimensions than the output.\n",
    "\n",
    "The class ResNetMLMed18 will inherit from torch.nn.Module, so we need to implement the init() and forward() functions. Using Table 1 and Figure 5 from [2], we define each part of resblocks2-5. The indexing follows the same convention as in Table 1, allowing for easy comparison of block numbers, kernel sizes, and number of channels.\n",
    "\n",
    "The DimsReduction block is the first block in resblocks2-5, as it performs downsampling"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "A few words about the torch.nn.init part:\n",
    "PyTorch initializes the parameters for Conv and batch norm randomly from uniform distribution. Initialization of the weights and biases with a normal distribution helps the model backpropagate gradients in early epochs.\n",
    "\n",
    "Tests were conducted on smaller models with 18, 34, and 50 layers, indicating that for adaptive optimizers, weight and bias initialization has minimal effect on model performance or convergence.\n",
    "\n",
    "In contrast, the uniform initialized ResNet 152 model exhibited poor convergence after 15 epochs, with very high error and low accuracy rates. Although initialization improved the performance, it still required tuning of hyperparameters and a better optimizer."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ResBlock Class\n",
    "#       - constructs a block [conv -> batch_norm -> activation] *2, which we will stack in the network\n",
    "# Input:    int: n_chans - number channels\n",
    "# Output:   nn.Sequential() block\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, n_chans):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(n_chans, n_chans, kernel_size=3, padding=1, bias= False)\n",
    "        self.batch_norm1 = nn.BatchNorm2d(num_features=n_chans)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.conv2 = nn.Conv2d(n_chans, n_chans, kernel_size=3, padding=1, bias= False)\n",
    "        self.batch_norm2 = nn.BatchNorm2d(num_features=n_chans)\n",
    "\n",
    "        torch.nn.init.kaiming_normal_(self.conv1.weight,\n",
    "                                      nonlinearity='relu')\n",
    "        torch.nn.init.kaiming_normal_(self.conv2.weight,\n",
    "                                      nonlinearity='relu')\n",
    "\n",
    "        torch.nn.init.constant_(self.batch_norm1.weight, 0.5)\n",
    "        torch.nn.init.zeros_(self.batch_norm1.bias)\n",
    "\n",
    "        torch.nn.init.constant_(self.batch_norm2.weight, 0.5)\n",
    "        torch.nn.init.zeros_(self.batch_norm2.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.batch_norm1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.batch_norm2(out)\n",
    "        out = self.relu(out)\n",
    "        return out + x # this sum realise the skip connection\n",
    "\n",
    "\n",
    "# ResBlockDimsReduction Class\n",
    "#       - constructs a first block in the layer\n",
    "#       - [conv -> batch_norm -> activation] *2\n",
    "#       - downsampling performed with stride 2\n",
    "# Input:    int: num_chans_in; int:num_chans_out\n",
    "# Output:   nn.Sequential() block\n",
    "\n",
    "class ResBlockDimsReduction(nn.Module):\n",
    "    def __init__(self, num_chans_in, num_chans_out):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(num_chans_in, num_chans_out, kernel_size=3, stride=2,padding=1,bias= False)\n",
    "        self.batch_norm1 = nn.BatchNorm2d(num_features=num_chans_out)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.conv2 = nn.Conv2d(num_chans_out, num_chans_out, kernel_size=3, padding=1, bias= False)\n",
    "        self.batch_norm2 = nn.BatchNorm2d(num_features=num_chans_out)\n",
    "\n",
    "        torch.nn.init.kaiming_normal_(self.conv1.weight,\n",
    "                                      nonlinearity='relu')\n",
    "        torch.nn.init.kaiming_normal_(self.conv2.weight,\n",
    "                                      nonlinearity='relu')\n",
    "        torch.nn.init.constant_(self.batch_norm1.weight, 0.5)\n",
    "        torch.nn.init.zeros_(self.batch_norm1.bias)\n",
    "        torch.nn.init.constant_(self.batch_norm2.weight, 0.5)\n",
    "        torch.nn.init.zeros_(self.batch_norm2.bias)\n",
    "\n",
    "        self.downsample = nn.Sequential(\n",
    "            nn.Conv2d(num_chans_in, num_chans_out, kernel_size=1, stride=2,bias= False),\n",
    "            nn.BatchNorm2d(num_features=num_chans_out),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.batch_norm1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.batch_norm2(out)\n",
    "        out = self.relu(out)\n",
    "        # input and output dimensions not match, so we need to project x into the dimensions of out\n",
    "        x = self.downsample(x)\n",
    "        return out + x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ResNetMLMed18 Class\n",
    "#       - constructs a ResNet34 as described [2, Table 1].\n",
    "# Input:    Tensor: [Batch,1,Height,Width]\n",
    "# Output:   Tensor: [Batch,3]\n",
    "class ResNetMLMed18(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = torch.nn.Conv2d(in_channels = 1, out_channels = 64, kernel_size =7, stride =2, padding=1)\n",
    "        self.batch_norm1 = nn.BatchNorm2d(num_features=64)\n",
    "        self.pool2 = torch.nn.MaxPool2d(kernel_size = 3, stride = 2)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "\n",
    "        self.resblocks2 =nn.Sequential(\n",
    "            *(2 * [ResBlock(n_chans=64)]))\n",
    "        self.resblocks3 = nn.Sequential(ResBlockDimsReduction(num_chans_in=64,num_chans_out=128),\n",
    "            *(1 * [ResBlock(n_chans=128)]))\n",
    "        self.resblocks4 = nn.Sequential(ResBlockDimsReduction(num_chans_in=128,num_chans_out=256),\n",
    "            *(1 * [ResBlock(n_chans=256)]))\n",
    "        self.resblocks5 = nn.Sequential(ResBlockDimsReduction(num_chans_in=256,num_chans_out=512),\n",
    "            *(1 * [ResBlock(n_chans=512)]))\n",
    "        self.avgpool6 = nn.AdaptiveAvgPool2d(output_size=(1, 1))\n",
    "        self.fc = nn.Linear(in_features=512, out_features=3, bias=True)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        out_1 = self.conv1(x)\n",
    "        out_1 = self.batch_norm1(out_1)\n",
    "        out_1 = self.relu(out_1)\n",
    "        out_1 = self.pool2(out_1)\n",
    "\n",
    "        out_2 = self.resblocks2(out_1)\n",
    "\n",
    "        out_3 = self.resblocks3(out_2)\n",
    "\n",
    "        out_4 = self.resblocks4(out_3)\n",
    "\n",
    "        out_5 = self.resblocks5(out_4)\n",
    "\n",
    "        out_6 = self.avgpool6(out_5)\n",
    "\n",
    "        out_6= self.fc(torch.flatten(out_6, start_dim=1))\n",
    "\n",
    "        return out_6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = ResNetMLMed18()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for step, (data, targets) in enumerate(dl):\n",
    "    data, targets = data.to(device), targets.to(device)\n",
    "    if step ==1:\n",
    "        break\n",
    "model(data).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "learning_rate = 1e-5\n",
    "run_name = \"ResNet18_fixed_lre5\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "uu.csv_logger(\n",
    "        logfile = f\"../logs/{run_name}_hyperparams.csv\",\n",
    "        content = {\"learning_rate\": learning_rate, \"batch_size\": batch_size, \"epochs\": epochs},\n",
    "        first= True,\n",
    "        overwrite= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mod_step = 5000\n",
    "wantToPrint = False\n",
    "stop_bool = False\n",
    "eval_test_10_min = False\n",
    "eval_test_15_min = False\n",
    "eval_test_20_min = False\n",
    "skip_test_10_min = False\n",
    "skip_test_15_min = False\n",
    "skip_test_20_min = False"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Modified training loop: The starting time is saved, and the time elapsed is calculated at the beginning of each epoch. If the elapsed time is greater than 10, 15, or 20 minutes, the boolean flag for test evaluation is set to True. However, it is important to ensure that the test evaluation happens only once. Therefore, after the calculation, the boolean flag for skipping is set to True.\n",
    "\n",
    "During the test evaluation, the dataset mode is switched to \"test\", the model is switched to evaluation mode, and the test accuracy, loss, confusion matrix, and per-class accuracy are calculated and saved.\n",
    "\n",
    "The same procedure is repeated three times for each time step."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_start = time.time()\n",
    "\n",
    "num_steps = len(ds.file_names['train'])//batch_size\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    time_elapsed = time.time() - train_start\n",
    "    print(f\"Time_elapsed: {time_elapsed/60 :.2f} min\")\n",
    "    if time_elapsed > 10*60:\n",
    "        eval_test_10_min = True\n",
    "    if time_elapsed > 15*60:\n",
    "        eval_test_15_min = True\n",
    "    if time_elapsed > 20*60:\n",
    "        eval_test_20_min = True\n",
    "\n",
    "    if eval_test_10_min and not skip_test_10_min:\n",
    "        print('Evaluate after first 10 min')\n",
    "        with torch.no_grad():\n",
    "            ds.set_mode(\"test\")\n",
    "            model.eval()\n",
    "            test_accuracy, avg_test_loss = evaluate_classifier_model(model = model, dataloader = dl, device = device)\n",
    "            confusion_matrix, acc = calculate_confusion_matrix(model=model, dataloader=dl, device=device)\n",
    "            torch.save(confusion_matrix, f = 'confusion_matr_' + run_name+ '_10min' + '.pt')\n",
    "            print(f\"Evaluate after first 10 min: Test Loss: {avg_test_loss:.4f}\\t Test Accuracy: {test_accuracy:.4f}, Confusion Matrix: \\n{confusion_matrix}, Per-class Accuracy: {acc}\")\n",
    "            uu.csv_logger(\n",
    "                logfile = f\"../logs/{run_name}_test.csv\",\n",
    "                content = {\"epoch\": epoch,\"test_phase\": 1, \"test_loss\": avg_test_loss, \"test_accuracy\": test_accuracy, \"time_elapsed\": time_elapsed})\n",
    "        skip_test_10_min = True\n",
    "\n",
    "    if eval_test_15_min and not skip_test_15_min:\n",
    "        print('Evaluate after first 15 min')\n",
    "        with torch.no_grad():\n",
    "            ds.set_mode(\"test\")\n",
    "            model.eval()\n",
    "            test_accuracy, avg_test_loss = evaluate_classifier_model(model = model, dataloader = dl, device = device)\n",
    "            confusion_matrix, acc = calculate_confusion_matrix(model=model, dataloader=dl, device=device)\n",
    "            torch.save(confusion_matrix, f = 'confusion_matr_' + run_name+ '_15min' + '.pt')\n",
    "            print(f\"Evaluate after first 15 min: Test Loss: {avg_test_loss:.4f}\\t Test Accuracy: {test_accuracy:.4f}, Confusion Matrix: \\n{confusion_matrix}, Per-class Accuracy: {acc}\")\n",
    "            uu.csv_logger(\n",
    "                logfile = f\"../logs/{run_name}_test.csv\",\n",
    "                content = {\"epoch\": epoch,\"test_phase\": 2, \"test_loss\": avg_test_loss, \"test_accuracy\": test_accuracy, \"time_elapsed\": time_elapsed})\n",
    "        skip_test_15_min = True\n",
    "\n",
    "    if eval_test_20_min and not skip_test_20_min:\n",
    "        print('Evaluate after first 20 min')\n",
    "        with torch.no_grad():\n",
    "            ds.set_mode(\"test\")\n",
    "            model.eval()\n",
    "            test_accuracy, avg_test_loss = evaluate_classifier_model(model = model, dataloader = dl, device = device)\n",
    "            confusion_matrix, acc = calculate_confusion_matrix(model=model, dataloader=dl, device=device)\n",
    "            torch.save(confusion_matrix, f = 'confusion_matr_' + run_name+ '_20min' + '.pt')\n",
    "            print(f\"Evaluate after first 20 min: Test Loss: {avg_test_loss:.4f}\\t Test Accuracy: {test_accuracy:.4f}, Confusion Matrix: \\n{confusion_matrix}, Per-class Accuracy: {acc}\")\n",
    "            uu.csv_logger(\n",
    "                logfile = f\"../logs/{run_name}_test.csv\",\n",
    "                content = {\"epoch\": epoch,\"test_phase\": 3, \"test_loss\": avg_test_loss, \"test_accuracy\": test_accuracy, \"time_elapsed\": time_elapsed})\n",
    "        skip_test_20_min = True\n",
    "\n",
    "        break\n",
    "\n",
    "    # Go to train mode\n",
    "    ds.set_mode(\"train\")\n",
    "    model.train()\n",
    "\n",
    "    # Train loop\n",
    "    for step, (data, targets) in enumerate(dl):\n",
    "\n",
    "        # Manually drop last batch (this is for example relevant with BatchNorm)\n",
    "        if step == num_steps - 1 and (epoch > 0 or ds.cache_data is False):\n",
    "            continue\n",
    "\n",
    "        # Train loop: Zero gradients, forward step, evaluate, log, backward step\n",
    "        optimizer.zero_grad()\n",
    "        data, targets = data.to(device), targets.to(device)\n",
    "        predictions = model(data)\n",
    "        loss = criterion(predictions, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Go to eval mode\n",
    "    ds.set_mode(\"val\")\n",
    "    model.eval()\n",
    "\n",
    "    # Validation loop\n",
    "    val_accuracy, avg_val_loss = evaluate_classifier_model(model = model, dataloader = dl, device = device)\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}]\\t Val Loss: {avg_val_loss:.4f}\\t Val Accuracy: {val_accuracy:.4f}\")\n",
    "    uu.csv_logger(\n",
    "        logfile = f\"../logs/{run_name}_val.csv\",\n",
    "        content = {\"epoch\": epoch, \"val_loss\": avg_val_loss, \"val_accuracy\": val_accuracy},\n",
    "        first = (epoch == 0),\n",
    "        overwrite = (epoch == 0)\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "run_name = \"ResNet18_fixed_time_lre4\"\n",
    "learning_rate = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "del model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = ResNetMLMed18()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stop_bool = False\n",
    "eval_test_10_min = False\n",
    "eval_test_15_min = False\n",
    "eval_test_20_min = False\n",
    "skip_test_10_min = False\n",
    "skip_test_15_min = False\n",
    "skip_test_20_min = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_start = time.time()\n",
    "\n",
    "num_steps = len(ds.file_names['train'])//batch_size\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    time_elapsed = time.time() - train_start\n",
    "    print(f\"Time_elapsed: {time_elapsed/60 :.2f} min\")\n",
    "    if time_elapsed > 10*60:\n",
    "        eval_test_10_min = True\n",
    "    if time_elapsed > 15*60:\n",
    "        eval_test_15_min = True\n",
    "    if time_elapsed > 20*60:\n",
    "        eval_test_20_min = True\n",
    "\n",
    "    if eval_test_10_min and not skip_test_10_min:\n",
    "        print('Evaluate after first 10 min')\n",
    "        with torch.no_grad():\n",
    "            ds.set_mode(\"test\")\n",
    "            model.eval()\n",
    "            test_accuracy, avg_test_loss = evaluate_classifier_model(model = model, dataloader = dl, device = device)\n",
    "            confusion_matrix, acc = calculate_confusion_matrix(model=model, dataloader=dl, device=device)\n",
    "            torch.save(confusion_matrix, f = 'confusion_matr_' + run_name+ '_10min' + '.pt')\n",
    "            print(f\"Evaluate after first 10 min: Test Loss: {avg_test_loss:.4f}\\t Test Accuracy: {test_accuracy:.4f}, Confusion Matrix: \\n{confusion_matrix}, Per-class Accuracy: {acc}\")\n",
    "            uu.csv_logger(\n",
    "                logfile = f\"../logs/{run_name}_test.csv\",\n",
    "                content = {\"epoch\": epoch,\"test_phase\": 1, \"test_loss\": avg_test_loss, \"test_accuracy\": test_accuracy, \"time_elapsed\": time_elapsed})\n",
    "        skip_test_10_min = True\n",
    "\n",
    "    if eval_test_15_min and not skip_test_15_min:\n",
    "        print('Evaluate after first 15 min')\n",
    "        with torch.no_grad():\n",
    "            ds.set_mode(\"test\")\n",
    "            model.eval()\n",
    "            test_accuracy, avg_test_loss = evaluate_classifier_model(model = model, dataloader = dl, device = device)\n",
    "            confusion_matrix, acc = calculate_confusion_matrix(model=model, dataloader=dl, device=device)\n",
    "            torch.save(confusion_matrix, f = 'confusion_matr_' + run_name+ '_15min' + '.pt')\n",
    "            print(f\"Evaluate after first 15 min: Test Loss: {avg_test_loss:.4f}\\t Test Accuracy: {test_accuracy:.4f}, Confusion Matrix: \\n{confusion_matrix}, Per-class Accuracy: {acc}\")\n",
    "            uu.csv_logger(\n",
    "                logfile = f\"../logs/{run_name}_test.csv\",\n",
    "                content = {\"epoch\": epoch,\"test_phase\": 2, \"test_loss\": avg_test_loss, \"test_accuracy\": test_accuracy, \"time_elapsed\": time_elapsed})\n",
    "        skip_test_15_min = True\n",
    "\n",
    "    if eval_test_20_min and not skip_test_20_min:\n",
    "        print('Evaluate after first 20 min')\n",
    "        with torch.no_grad():\n",
    "            ds.set_mode(\"test\")\n",
    "            model.eval()\n",
    "            test_accuracy, avg_test_loss = evaluate_classifier_model(model = model, dataloader = dl, device = device)\n",
    "            confusion_matrix, acc = calculate_confusion_matrix(model=model, dataloader=dl, device=device)\n",
    "            torch.save(confusion_matrix, f = 'confusion_matr_' + run_name+ '_20min' + '.pt')\n",
    "            print(f\"Evaluate after first 20 min: Test Loss: {avg_test_loss:.4f}\\t Test Accuracy: {test_accuracy:.4f}, Confusion Matrix: \\n{confusion_matrix}, Per-class Accuracy: {acc}\")\n",
    "            uu.csv_logger(\n",
    "                logfile = f\"../logs/{run_name}_test.csv\",\n",
    "                content = {\"epoch\": epoch,\"test_phase\": 3, \"test_loss\": avg_test_loss, \"test_accuracy\": test_accuracy, \"time_elapsed\": time_elapsed})\n",
    "        skip_test_20_min = True\n",
    "\n",
    "        break\n",
    "\n",
    "    # Go to train mode\n",
    "    ds.set_mode(\"train\")\n",
    "    model.train()\n",
    "\n",
    "    # Train loop\n",
    "    for step, (data, targets) in enumerate(dl):\n",
    "\n",
    "        # Manually drop last batch (this is for example relevant with BatchNorm)\n",
    "        if step == num_steps - 1 and (epoch > 0 or ds.cache_data is False):\n",
    "            continue\n",
    "\n",
    "        # Train loop: Zero gradients, forward step, evaluate, log, backward step\n",
    "        optimizer.zero_grad()\n",
    "        data, targets = data.to(device), targets.to(device)\n",
    "        predictions = model(data)\n",
    "        loss = criterion(predictions, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Go to eval mode\n",
    "    ds.set_mode(\"val\")\n",
    "    model.eval()\n",
    "\n",
    "    # Validation loop\n",
    "    val_accuracy, avg_val_loss = evaluate_classifier_model(model = model, dataloader = dl, device = device)\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}]\\t Val Loss: {avg_val_loss:.4f}\\t Val Accuracy: {val_accuracy:.4f}\")\n",
    "    uu.csv_logger(\n",
    "        logfile = f\"../logs/{run_name}_val.csv\",\n",
    "        content = {\"epoch\": epoch, \"val_loss\": avg_val_loss, \"val_accuracy\": val_accuracy},\n",
    "        first = (epoch == 0),\n",
    "        overwrite = (epoch == 0)\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "run_name = \"ResNet18_fixed_lre3\"\n",
    "learning_rate = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "del model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = ResNetMLMed18()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stop_bool = False\n",
    "eval_test_10_min = False\n",
    "eval_test_15_min = False\n",
    "eval_test_20_min = False\n",
    "skip_test_10_min = False\n",
    "skip_test_15_min = False\n",
    "skip_test_20_min = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_start = time.time()\n",
    "\n",
    "num_steps = len(ds.file_names['train'])//batch_size\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    time_elapsed = time.time() - train_start\n",
    "    print(f\"Time_elapsed: {time_elapsed/60 :.2f} min\")\n",
    "    if time_elapsed > 10*60:\n",
    "        eval_test_10_min = True\n",
    "    if time_elapsed > 15*60:\n",
    "        eval_test_15_min = True\n",
    "    if time_elapsed > 20*60:\n",
    "        eval_test_20_min = True\n",
    "\n",
    "    if eval_test_10_min and not skip_test_10_min:\n",
    "        print('Evaluate after first 10 min')\n",
    "        with torch.no_grad():\n",
    "            ds.set_mode(\"test\")\n",
    "            model.eval()\n",
    "            test_accuracy, avg_test_loss = evaluate_classifier_model(model = model, dataloader = dl, device = device)\n",
    "            confusion_matrix, acc = calculate_confusion_matrix(model=model, dataloader=dl, device=device)\n",
    "            torch.save(confusion_matrix, f = 'confusion_matr_' + run_name+ '_10min' + '.pt')\n",
    "            print(f\"Evaluate after first 10 min: Test Loss: {avg_test_loss:.4f}\\t Test Accuracy: {test_accuracy:.4f}, Confusion Matrix: \\n{confusion_matrix}, Per-class Accuracy: {acc}\")\n",
    "            uu.csv_logger(\n",
    "                logfile = f\"../logs/{run_name}_test.csv\",\n",
    "                content = {\"epoch\": epoch,\"test_phase\": 1, \"test_loss\": avg_test_loss, \"test_accuracy\": test_accuracy, \"time_elapsed\": time_elapsed})\n",
    "        skip_test_10_min = True\n",
    "\n",
    "    if eval_test_15_min and not skip_test_15_min:\n",
    "        print('Evaluate after first 15 min')\n",
    "        with torch.no_grad():\n",
    "            ds.set_mode(\"test\")\n",
    "            model.eval()\n",
    "            test_accuracy, avg_test_loss = evaluate_classifier_model(model = model, dataloader = dl, device = device)\n",
    "            confusion_matrix, acc = calculate_confusion_matrix(model=model, dataloader=dl, device=device)\n",
    "            torch.save(confusion_matrix, f = 'confusion_matr_' + run_name+ '_15min' + '.pt')\n",
    "            print(f\"Evaluate after first 15 min: Test Loss: {avg_test_loss:.4f}\\t Test Accuracy: {test_accuracy:.4f}, Confusion Matrix: \\n{confusion_matrix}, Per-class Accuracy: {acc}\")\n",
    "            uu.csv_logger(\n",
    "                logfile = f\"../logs/{run_name}_test.csv\",\n",
    "                content = {\"epoch\": epoch,\"test_phase\": 2, \"test_loss\": avg_test_loss, \"test_accuracy\": test_accuracy, \"time_elapsed\": time_elapsed})\n",
    "        skip_test_15_min = True\n",
    "\n",
    "    if eval_test_20_min and not skip_test_20_min:\n",
    "        print('Evaluate after first 20 min')\n",
    "        with torch.no_grad():\n",
    "            ds.set_mode(\"test\")\n",
    "            model.eval()\n",
    "            test_accuracy, avg_test_loss = evaluate_classifier_model(model = model, dataloader = dl, device = device)\n",
    "            confusion_matrix, acc = calculate_confusion_matrix(model=model, dataloader=dl, device=device)\n",
    "            torch.save(confusion_matrix, f = 'confusion_matr_' + run_name+ '_20min' + '.pt')\n",
    "            print(f\"Evaluate after first 20 min: Test Loss: {avg_test_loss:.4f}\\t Test Accuracy: {test_accuracy:.4f}, Confusion Matrix: \\n{confusion_matrix}, Per-class Accuracy: {acc}\")\n",
    "            uu.csv_logger(\n",
    "                logfile = f\"../logs/{run_name}_test.csv\",\n",
    "                content = {\"epoch\": epoch,\"test_phase\": 3, \"test_loss\": avg_test_loss, \"test_accuracy\": test_accuracy, \"time_elapsed\": time_elapsed})\n",
    "        skip_test_20_min = True\n",
    "\n",
    "        break\n",
    "\n",
    "    # Go to train mode\n",
    "    ds.set_mode(\"train\")\n",
    "    model.train()\n",
    "\n",
    "    # Train loop\n",
    "    for step, (data, targets) in enumerate(dl):\n",
    "\n",
    "        # Manually drop last batch (this is for example relevant with BatchNorm)\n",
    "        if step == num_steps - 1 and (epoch > 0 or ds.cache_data is False):\n",
    "            continue\n",
    "\n",
    "        # Train loop: Zero gradients, forward step, evaluate, log, backward step\n",
    "        optimizer.zero_grad()\n",
    "        data, targets = data.to(device), targets.to(device)\n",
    "        predictions = model(data)\n",
    "        loss = criterion(predictions, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Go to eval mode\n",
    "    ds.set_mode(\"val\")\n",
    "    model.eval()\n",
    "\n",
    "    # Validation loop\n",
    "    val_accuracy, avg_val_loss = evaluate_classifier_model(model = model, dataloader = dl, device = device)\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}]\\t Val Loss: {avg_val_loss:.4f}\\t Val Accuracy: {val_accuracy:.4f}\")\n",
    "    uu.csv_logger(\n",
    "        logfile = f\"../logs/{run_name}_val.csv\",\n",
    "        content = {\"epoch\": epoch, \"val_loss\": avg_val_loss, \"val_accuracy\": val_accuracy},\n",
    "        first = (epoch == 0),\n",
    "        overwrite = (epoch == 0)\n",
    "            )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlcourse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5555b6f2b0077a97828fbbfe12cb97727895c9c472121c1b71224aa97370345d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
